{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FOUvgVp3xnNW"
   },
   "source": [
    "# Numerai Modeling: Feature Engineering, Ensembling, and Advanced Training\n",
    "\n",
    "This notebook demonstrates several advanced modeling techniques for the Numerai tournament:\n",
    "1. **Feature Engineering**: Creating new features using UMAP, Denoising Autoencoders, Contrastive Learning (placeholder), and CTGAN.\n",
    "2. **Target Exploration**: Analyzing auxiliary targets.\n",
    "3. **Base Model Training**: Training LightGBM models on different targets and engineered features.\n",
    "4. **Stacked Ensembling**: Combining base model predictions using a meta-model.\n",
    "5. **Era-Invariant Training**: Using a PyTorch MLP with custom loss functions (correlation, era variance penalty, feature exposure penalty).\n",
    "6. **Model Selection & Upload**: Choosing the final model and preparing for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KD826S8uxnNY",
    "outputId": "418275f7-0333-4e08-db73-2407cc9afe3e"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Error importing numpy: you should not try to import numpy from\n        its source directory; please exit the numpy source tree, and relaunch\n        your python interpreter from there.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lightgbm-env/lib/python3.11/site-packages/numpy/_core/__init__.py:23\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m multiarray\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lightgbm-env/lib/python3.11/site-packages/numpy/_core/multiarray.py:10\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfunctools\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m overrides\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _multiarray_umath\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lightgbm-env/lib/python3.11/site-packages/numpy/_core/overrides.py:8\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_utils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_inspect\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m getargspec\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_multiarray_umath\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      9\u001b[39m     add_docstring,  _get_implementing_args, _ArrayFunctionDispatcher)\n\u001b[32m     12\u001b[39m ARRAY_FUNCTIONS = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[31mImportError\u001b[39m: dlopen(/Users/I570611/miniconda3/envs/lightgbm-env/lib/python3.11/site-packages/numpy/_core/_multiarray_umath.cpython-311-darwin.so, 0x0002): tried: '/Users/I570611/miniconda3/envs/lightgbm-env/lib/python3.11/site-packages/numpy/_core/_multiarray_umath.cpython-311-darwin.so' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64')), '/System/Volumes/Preboot/Cryptexes/OS/Users/I570611/miniconda3/envs/lightgbm-env/lib/python3.11/site-packages/numpy/_core/_multiarray_umath.cpython-311-darwin.so' (no such file), '/Users/I570611/miniconda3/envs/lightgbm-env/lib/python3.11/site-packages/numpy/_core/_multiarray_umath.cpython-311-darwin.so' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lightgbm-env/lib/python3.11/site-packages/numpy/__init__.py:114\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m__config__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m show \u001b[38;5;28;01mas\u001b[39;00m show_config\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lightgbm-env/lib/python3.11/site-packages/numpy/__config__.py:4\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01menum\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Enum\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_multiarray_umath\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      5\u001b[39m     __cpu_features__,\n\u001b[32m      6\u001b[39m     __cpu_baseline__,\n\u001b[32m      7\u001b[39m     __cpu_dispatch__,\n\u001b[32m      8\u001b[39m )\n\u001b[32m     10\u001b[39m __all__ = [\u001b[33m\"\u001b[39m\u001b[33mshow\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lightgbm-env/lib/python3.11/site-packages/numpy/_core/__init__.py:49\u001b[39m\n\u001b[32m     26\u001b[39m     msg = \u001b[33m\"\"\"\u001b[39m\n\u001b[32m     27\u001b[39m \n\u001b[32m     28\u001b[39m \u001b[33mIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     47\u001b[39m \u001b[33m\"\"\"\u001b[39m % (sys.version_info[\u001b[32m0\u001b[39m], sys.version_info[\u001b[32m1\u001b[39m], sys.executable,\n\u001b[32m     48\u001b[39m         __version__, exc)\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[31mImportError\u001b[39m: \n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.11 from \"/Users/I570611/miniconda3/envs/lightgbm-env/bin/python\"\n  * The NumPy version is: \"2.0.1\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: dlopen(/Users/I570611/miniconda3/envs/lightgbm-env/lib/python3.11/site-packages/numpy/_core/_multiarray_umath.cpython-311-darwin.so, 0x0002): tried: '/Users/I570611/miniconda3/envs/lightgbm-env/lib/python3.11/site-packages/numpy/_core/_multiarray_umath.cpython-311-darwin.so' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64')), '/System/Volumes/Preboot/Cryptexes/OS/Users/I570611/miniconda3/envs/lightgbm-env/lib/python3.11/site-packages/numpy/_core/_multiarray_umath.cpython-311-darwin.so' (no such file), '/Users/I570611/miniconda3/envs/lightgbm-env/lib/python3.11/site-packages/numpy/_core/_multiarray_umath.cpython-311-darwin.so' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64'))\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m get_ipython().system(\u001b[33m'\u001b[39m\u001b[33mpip install -q numerapi pandas pyarrow matplotlib lightgbm scikit-learn cloudpickle==2.2.1 seaborn umap-learn tensorflow torch ctgan tqdm\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Inline plots\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_line_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmatplotlib\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minline\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lightgbm-env/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2486\u001b[39m, in \u001b[36mInteractiveShell.run_line_magic\u001b[39m\u001b[34m(self, magic_name, line, _stack_depth)\u001b[39m\n\u001b[32m   2484\u001b[39m     kwargs[\u001b[33m'\u001b[39m\u001b[33mlocal_ns\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mself\u001b[39m.get_local_scope(stack_depth)\n\u001b[32m   2485\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.builtin_trap:\n\u001b[32m-> \u001b[39m\u001b[32m2486\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2488\u001b[39m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[32m   2489\u001b[39m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[32m   2490\u001b[39m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[32m   2491\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lightgbm-env/lib/python3.11/site-packages/IPython/core/magics/pylab.py:103\u001b[39m, in \u001b[36mPylabMagics.matplotlib\u001b[39m\u001b[34m(self, line)\u001b[39m\n\u001b[32m     98\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m     99\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAvailable matplotlib backends: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    100\u001b[39m         % _list_matplotlib_backends_and_gui_loops()\n\u001b[32m    101\u001b[39m     )\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m     gui, backend = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshell\u001b[49m\u001b[43m.\u001b[49m\u001b[43menable_matplotlib\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgui\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    104\u001b[39m     \u001b[38;5;28mself\u001b[39m._show_matplotlib_backend(args.gui, backend)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lightgbm-env/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3758\u001b[39m, in \u001b[36mInteractiveShell.enable_matplotlib\u001b[39m\u001b[34m(self, gui)\u001b[39m\n\u001b[32m   3755\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib_inline\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend_inline\u001b[39;00m\n\u001b[32m   3757\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pylabtools \u001b[38;5;28;01mas\u001b[39;00m pt\n\u001b[32m-> \u001b[39m\u001b[32m3758\u001b[39m gui, backend = \u001b[43mpt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfind_gui_and_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgui\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpylab_gui_select\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3760\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m gui != \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3761\u001b[39m     \u001b[38;5;66;03m# If we have our first gui selection, store it\u001b[39;00m\n\u001b[32m   3762\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pylab_gui_select \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lightgbm-env/lib/python3.11/site-packages/IPython/core/pylabtools.py:338\u001b[39m, in \u001b[36mfind_gui_and_backend\u001b[39m\u001b[34m(gui, gui_select)\u001b[39m\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfind_gui_and_backend\u001b[39m(gui=\u001b[38;5;28;01mNone\u001b[39;00m, gui_select=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    322\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Given a gui string return the gui and mpl backend.\u001b[39;00m\n\u001b[32m    323\u001b[39m \n\u001b[32m    324\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    335\u001b[39m \u001b[33;03m    'WXAgg','Qt4Agg','module://matplotlib_inline.backend_inline','agg').\u001b[39;00m\n\u001b[32m    336\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\n\u001b[32m    340\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _matplotlib_manages_backends():\n\u001b[32m    341\u001b[39m         backend_registry = matplotlib.backends.registry.backend_registry\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lightgbm-env/lib/python3.11/site-packages/matplotlib/__init__.py:161\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpackaging\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parse \u001b[38;5;28;01mas\u001b[39;00m parse_version\n\u001b[32m    159\u001b[39m \u001b[38;5;66;03m# cbook must import matplotlib only within function\u001b[39;00m\n\u001b[32m    160\u001b[39m \u001b[38;5;66;03m# definitions, so it is safe to import from it here.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _api, _version, cbook, _docstring, rcsetup\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MatplotlibDeprecationWarning\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrcsetup\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cycler  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lightgbm-env/lib/python3.11/site-packages/matplotlib/cbook.py:24\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtypes\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mweakref\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m VisibleDeprecationWarning  \u001b[38;5;66;03m# numpy >= 1.25\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lightgbm-env/lib/python3.11/site-packages/numpy/__init__.py:119\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    116\u001b[39m     msg = \u001b[33m\"\"\"\u001b[39m\u001b[33mError importing numpy: you should not try to import numpy from\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[33m    its source directory; please exit the numpy source tree, and relaunch\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[33m    your python interpreter from there.\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _core\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_core\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    123\u001b[39m     False_, ScalarType, True_, _get_promotion_state, _no_nep50_warning,\n\u001b[32m    124\u001b[39m     _set_promotion_state, \u001b[38;5;28mabs\u001b[39m, absolute, acos, acosh, add, \u001b[38;5;28mall\u001b[39m, allclose,\n\u001b[32m   (...)\u001b[39m\u001b[32m    169\u001b[39m     where, zeros, zeros_like\n\u001b[32m    170\u001b[39m )\n",
      "\u001b[31mImportError\u001b[39m: Error importing numpy: you should not try to import numpy from\n        its source directory; please exit the numpy source tree, and relaunch\n        your python interpreter from there."
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "# Removed scipy version pin to potentially resolve numpy import errors\n",
    "!pip install -q numerapi pandas pyarrow matplotlib lightgbm scikit-learn cloudpickle==2.2.1 seaborn umap-learn tensorflow torch ctgan tqdm\n",
    "\n",
    "# Inline plots\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config_cell"
   },
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "config_vars"
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "dlopen(/Users/I570611/miniconda3/envs/lightgbm-env/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.dylib, 0x0006): Library not loaded: @rpath/libomp.dylib\n  Referenced from: <8FC36893-94B8-343C-9D9F-4CCBFE81B89B> /Users/I570611/miniconda3/envs/lightgbm-env/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.dylib\n  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/opt/homebrew/opt/libomp/lib/libomp.dylib' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64')), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/opt/local/lib/libomp/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/local/lib/libomp/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/opt/homebrew/opt/libomp/lib/libomp.dylib' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64')), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/opt/local/lib/libomp/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/local/lib/libomp/libomp.dylib' (no such file), '/Users/I570611/miniconda3/envs/lightgbm-env/lib/python3.11/lib-dynload/../../libomp.dylib' (no such file), '/Users/I570611/miniconda3/envs/lightgbm-env/bin/../lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgc\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumerapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NumerAPI\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlightgbm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlgb\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GroupKFold\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinear_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Ridge\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lightgbm-env/lib/python3.11/site-packages/lightgbm/__init__.py:11\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# .basic is intentionally loaded as early as possible, to dlopen() lib_lightgbm.{dll,dylib,so}\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# and its dependencies as early as possible\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbasic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Booster, Dataset, Sequence, register_logger\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcallback\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EarlyStopException, early_stopping, log_evaluation, record_evaluation, reset_parameter\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mengine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CVBooster, cv, train\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lightgbm-env/lib/python3.11/site-packages/lightgbm/basic.py:9\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[33;03m\"\"\"Wrapper for C API of LightGBM.\"\"\"\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# This import causes lib_lightgbm.{dll,dylib,so} to be loaded.\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# It's intentionally done here, as early as possible, to avoid issues like\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# \"libgomp.so.1: cannot allocate memory in static TLS block\" on aarch64 Linux.\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# For details, see the \"cannot allocate memory in static TLS block\" entry in docs/FAQ.rst.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlibpath\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _LIB  \u001b[38;5;66;03m# isort: skip\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mabc\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mctypes\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lightgbm-env/lib/python3.11/site-packages/lightgbm/libpath.py:49\u001b[39m\n\u001b[32m     47\u001b[39m     _LIB = Mock(ctypes.CDLL)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     _LIB = \u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcdll\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLoadLibrary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_find_lib_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lightgbm-env/lib/python3.11/ctypes/__init__.py:454\u001b[39m, in \u001b[36mLibraryLoader.LoadLibrary\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mLoadLibrary\u001b[39m(\u001b[38;5;28mself\u001b[39m, name):\n\u001b[32m--> \u001b[39m\u001b[32m454\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dlltype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lightgbm-env/lib/python3.11/ctypes/__init__.py:376\u001b[39m, in \u001b[36mCDLL.__init__\u001b[39m\u001b[34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[39m\n\u001b[32m    373\u001b[39m \u001b[38;5;28mself\u001b[39m._FuncPtr = _FuncPtr\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m376\u001b[39m     \u001b[38;5;28mself\u001b[39m._handle = \u001b[43m_dlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    378\u001b[39m     \u001b[38;5;28mself\u001b[39m._handle = handle\n",
      "\u001b[31mOSError\u001b[39m: dlopen(/Users/I570611/miniconda3/envs/lightgbm-env/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.dylib, 0x0006): Library not loaded: @rpath/libomp.dylib\n  Referenced from: <8FC36893-94B8-343C-9D9F-4CCBFE81B89B> /Users/I570611/miniconda3/envs/lightgbm-env/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.dylib\n  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/opt/homebrew/opt/libomp/lib/libomp.dylib' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64')), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/opt/local/lib/libomp/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/local/lib/libomp/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/opt/homebrew/opt/libomp/lib/libomp.dylib' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64')), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/opt/local/lib/libomp/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/local/lib/libomp/libomp.dylib' (no such file), '/Users/I570611/miniconda3/envs/lightgbm-env/lib/python3.11/lib-dynload/../../libomp.dylib' (no such file), '/Users/I570611/miniconda3/envs/lightgbm-env/bin/../lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import gc\n",
    "from numerapi import NumerAPI\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer\n",
    "import cloudpickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "\n",
    "# Ignore specific warnings if needed (e.g., from CTGAN or TF)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='sklearn.preprocessing._data')\n",
    "\n",
    "# --- Configuration ---\n",
    "DATA_VERSION = \"v5.0\"\n",
    "MAIN_TARGET = \"target_cyrusd_20\"\n",
    "AUX_TARGETS = [\n",
    "  \"target_victor_20\",\n",
    "  \"target_xerxes_20\",\n",
    "  \"target_teager2b_20\"\n",
    "]\n",
    "TARGET_CANDIDATES = [MAIN_TARGET] + AUX_TARGETS\n",
    "ERA_COL = \"era\"\n",
    "DATA_TYPE_COL = \"data_type\"\n",
    "TARGET_COL = \"target\" # Alias for MAIN_TARGET in original notebook\n",
    "PREDICTION_COL = \"prediction\"\n",
    "\n",
    "# Feature Engineering Hyperparameters\n",
    "UMAP_N_COMPONENTS = 50\n",
    "AE_ENCODING_DIM = 64\n",
    "CONTRASTIVE_EMB_DIM = 64\n",
    "CTGAN_EPOCHS = 50 # Reduced further for speed in example\n",
    "AE_EPOCHS = 5 # Reduced further for speed in example\n",
    "\n",
    "# Stacking Ensemble Config\n",
    "N_FOLDS = 5 # Number of folds for OOF predictions\n",
    "STACKING_MODEL_TYPE = 'LGBM' # 'LGBM' or 'Linear'\n",
    "\n",
    "# PyTorch MLP Config\n",
    "MLP_EPOCHS = 5 # Reduced further for speed in example\n",
    "MLP_BATCH_SIZE = 1024 # Increased batch size might speed up training on GPU\n",
    "MLP_LR = 0.001\n",
    "VARIANCE_PENALTY_WEIGHT = 0.01 # lambda1\n",
    "FEATURE_EXPOSURE_WEIGHT = 0.01 # lambda2\n",
    "TOP_N_FEATURES_FOR_EXPOSURE = 50 # Use top N features for exposure penalty\n",
    "\n",
    "# Model Selection Flags\n",
    "USE_STACKING = True # Set to True to use Stacking, False for MLP\n",
    "USE_MLP = False      # Set to True to use MLP (requires PyTorch)\n",
    "\n",
    "# Speedup Options (Highly Recommended for faster iteration)\n",
    "DOWNSAMPLE_TRAIN_ERAS = 4 # Use every Nth era for training (e.g., 4 or 10)\n",
    "DOWNSAMPLE_VALID_ERAS = 4 # Use every Nth era for validation (e.g., 4 or 10)\n",
    "FEATURE_SET_SIZE = \"small\" # 'small', 'medium', or 'all'\n",
    "\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', lambda x: f'{x:.6f}')\n",
    "\n",
    "# Add comment about GPU potential\n",
    "# For significant speedups, especially with AE, CTGAN, and MLP, ensure you are running\n",
    "# in an environment with a GPU and have the necessary GPU versions of \n",
    "# TensorFlow and PyTorch installed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "feature_eng_funcs_md"
   },
   "source": [
    "## 1. Feature Engineering Functions\n",
    "\n",
    "Define functions to generate new features. These functions now fit transformers/models on the input data and return both the modified DataFrame and the fitted objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "feature_eng_funcs_code"
   },
   "outputs": [],
   "source": [
    "import umap\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from ctgan import CTGAN\n",
    "\n",
    "# --- UMAP Feature Creation ---\n",
    "def umap_feature_creation(df_train, df_transform, feature_cols, n_components=UMAP_N_COMPONENTS, random_state=42):\n",
    "    \"\"\"Fits UMAP on df_train and transforms both df_train and df_transform.\"\"\"\n",
    "    print(f\"Creating {n_components} UMAP features...\")\n",
    "    reducer = umap.UMAP(n_components=n_components, random_state=random_state, n_jobs=1) # n_jobs=1 can prevent potential issues\n",
    "    \n",
    "    # Fit on training data\n",
    "    print(\" Fitting UMAP on training data...\")\n",
    "    train_data = df_train[feature_cols].astype(np.float32).fillna(0.5)\n",
    "    reducer.fit(train_data)\n",
    "    \n",
    "    # Transform training data\n",
    "    print(\" Transforming training data...\")\n",
    "    umap_features_train = reducer.transform(train_data)\n",
    "    umap_feature_names = [f\"umap_feat_{i}\" for i in range(n_components)]\n",
    "    df_train[umap_feature_names] = umap_features_train\n",
    "    \n",
    "    # Transform the second dataframe (validation or live)\n",
    "    print(\" Transforming second dataframe...\")\n",
    "    transform_data = df_transform[feature_cols].astype(np.float32).fillna(0.5)\n",
    "    umap_features_transform = reducer.transform(transform_data)\n",
    "    df_transform[umap_feature_names] = umap_features_transform\n",
    "\n",
    "    print(\"UMAP features created and applied.\")\n",
    "    return df_train, df_transform, umap_feature_names, reducer\n",
    "\n",
    "# --- Denoising Autoencoder Feature Creation ---\n",
    "def denoising_autoencoder_features(df_train, df_transform, feature_cols, encoding_dim=AE_ENCODING_DIM, noise_factor=0.1, epochs=AE_EPOCHS, batch_size=1024):\n",
    "    \"\"\"Fits AE on df_train and transforms both df_train and df_transform.\"\"\"\n",
    "    print(f\"Creating {encoding_dim} Denoising AE features...\")\n",
    "    input_dim = len(feature_cols)\n",
    "    train_data = df_train[feature_cols].astype(np.float32).fillna(0.5).values\n",
    "\n",
    "    # Add noise to training data for denoising objective\n",
    "    noisy_train_data = train_data + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=train_data.shape)\n",
    "    noisy_train_data = np.clip(noisy_train_data, 0., 1.)\n",
    "\n",
    "    # Define Autoencoder\n",
    "    input_layer = keras.Input(shape=(input_dim,))\n",
    "    # Simple encoder-decoder structure\n",
    "    encoded = layers.Dense(encoding_dim * 2, activation='relu')(input_layer)\n",
    "    encoded = layers.Dense(encoding_dim, activation='relu', name='encoder_output')(encoded) # Naming the layer\n",
    "    decoded = layers.Dense(encoding_dim * 2, activation='relu')(encoded)\n",
    "    decoded = layers.Dense(input_dim, activation='sigmoid')(decoded)\n",
    "\n",
    "    autoencoder = keras.Model(input_layer, decoded)\n",
    "    encoder = keras.Model(input_layer, encoded) # Separate encoder model\n",
    "\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    # Train Autoencoder on noisy training data to reconstruct original\n",
    "    print(\" Training Denoising Autoencoder...\")\n",
    "    autoencoder.fit(noisy_train_data, train_data,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    validation_split=0.1,\n",
    "                    verbose=0)\n",
    "\n",
    "    # Get encoded features for training data\n",
    "    print(\" Encoding training data...\")\n",
    "    ae_features_train = encoder.predict(train_data)\n",
    "    ae_feature_names = [f\"ae_feat_{i}\" for i in range(encoding_dim)]\n",
    "    df_train[ae_feature_names] = ae_features_train\n",
    "\n",
    "    # Get encoded features for the second dataframe\n",
    "    print(\" Encoding second dataframe...\")\n",
    "    transform_data = df_transform[feature_cols].astype(np.float32).fillna(0.5).values\n",
    "    ae_features_transform = encoder.predict(transform_data)\n",
    "    df_transform[ae_feature_names] = ae_features_transform\n",
    "\n",
    "    print(\"AE features created and applied.\")\n",
    "    return df_train, df_transform, ae_feature_names, encoder\n",
    "\n",
    "# --- Contrastive Learning Feature Creation (Placeholder) ---\n",
    "def contrastive_feature_creation(df_train, df_transform, feature_cols, embedding_dim=CONTRASTIVE_EMB_DIM):\n",
    "    \"\"\"Placeholder: Generates random features for both dataframes.\"\"\"\n",
    "    print(f\"Creating {embedding_dim} Contrastive (placeholder) features...\")\n",
    "    # Proper implementation is complex and data-dependent.\n",
    "    num_samples_train = len(df_train)\n",
    "    num_samples_transform = len(df_transform)\n",
    "    contrastive_features_train = np.random.rand(num_samples_train, embedding_dim).astype(np.float32)\n",
    "    contrastive_features_transform = np.random.rand(num_samples_transform, embedding_dim).astype(np.float32)\n",
    "    \n",
    "    contrastive_feature_names = [f\"contrastive_feat_{i}\" for i in range(embedding_dim)]\n",
    "    df_train[contrastive_feature_names] = contrastive_features_train\n",
    "    df_transform[contrastive_feature_names] = contrastive_features_transform\n",
    "    print(\"Contrastive (placeholder) features created.\")\n",
    "    # No fitted object returned for this placeholder\n",
    "    return df_train, df_transform, contrastive_feature_names, None \n",
    "\n",
    "# --- CTGAN Feature Creation ---\n",
    "def synthetic_data_ctgan(df_train, df_transform, feature_cols, target_col, n_synthetic_samples_ratio=0.5, epochs=CTGAN_EPOCHS):\n",
    "    \"\"\"Fits CTGAN on df_train and generates a distance feature for both dataframes.\"\"\"\n",
    "    print(f\"Creating synthetic features using CTGAN (target: {target_col})...\")\n",
    "    \n",
    "    data_subset_train = df_train[feature_cols + [target_col]].copy().dropna(subset=[target_col])\n",
    "    data_subset_train[feature_cols] = data_subset_train[feature_cols].fillna(0.5)\n",
    "\n",
    "    # Use QuantileTransformer\n",
    "    qt = QuantileTransformer(output_distribution='uniform', random_state=42) # Uniform might work better for CTGAN\n",
    "    print(\" Fitting QuantileTransformer...\")\n",
    "    data_transformed_train = qt.fit_transform(data_subset_train[feature_cols])\n",
    "    data_transformed_df_train = pd.DataFrame(data_transformed_train, columns=feature_cols, index=data_subset_train.index)\n",
    "    data_transformed_df_train[target_col] = data_subset_train[target_col].values\n",
    "\n",
    "    discrete_columns = [] # Assuming no discrete columns for now\n",
    "\n",
    "    # Train CTGAN\n",
    "    print(\" Training CTGAN...\")\n",
    "    ctgan_model = CTGAN(verbose=False)\n",
    "    try:\n",
    "        ctgan_model.fit(data_transformed_df_train, discrete_columns, epochs=epochs)\n",
    "    except Exception as e:\n",
    "        print(f\"CTGAN fitting failed: {e}. Skipping CTGAN features.\")\n",
    "        return df_train, df_transform, [], None, None, None # Return Nones for objects\n",
    "\n",
    "    # Generate synthetic samples\n",
    "    print(\" Generating synthetic data...\")\n",
    "    n_synthetic_samples = int(len(data_subset_train) * n_synthetic_samples_ratio)\n",
    "    synthetic_data_transformed = ctgan_model.sample(n_synthetic_samples)\n",
    "\n",
    "    # Inverse transform synthetic features\n",
    "    synthetic_features_original_scale = qt.inverse_transform(synthetic_data_transformed[feature_cols])\n",
    "    synthetic_df = pd.DataFrame(synthetic_features_original_scale, columns=feature_cols)\n",
    "    \n",
    "    # Calculate mean of synthetic features\n",
    "    synthetic_mean = synthetic_df.mean(axis=0).astype(np.float32)\n",
    "    ctgan_feature_name = f\"dist_to_synth_mean_{target_col}\"\n",
    "    \n",
    "    # Calculate distance feature for training data\n",
    "    print(\" Calculating distance feature for training data...\")\n",
    "    original_features_train = df_train[feature_cols].fillna(0.5).values.astype(np.float32)\n",
    "    distances_train = np.linalg.norm(original_features_train - synthetic_mean.values, axis=1)\n",
    "    df_train[ctgan_feature_name] = distances_train\n",
    "\n",
    "    # Calculate distance feature for the second dataframe\n",
    "    print(\" Calculating distance feature for second dataframe...\")\n",
    "    original_features_transform = df_transform[feature_cols].fillna(0.5).values.astype(np.float32)\n",
    "    distances_transform = np.linalg.norm(original_features_transform - synthetic_mean.values, axis=1)\n",
    "    df_transform[ctgan_feature_name] = distances_transform\n",
    "\n",
    "    print(\"CTGAN-derived features created and applied.\")\n",
    "    \n",
    "    del data_subset_train, data_transformed_train, data_transformed_df_train, synthetic_data_transformed, synthetic_features_original_scale, synthetic_df\n",
    "    gc.collect()\n",
    "    \n",
    "    return df_train, df_transform, [ctgan_feature_name], ctgan_model, qt, synthetic_mean\n",
    "\n",
    "print(\"Feature engineering functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VwChLrKexnNa"
   },
   "source": [
    "## 2. Data Loading and Initial Exploration\n",
    "\n",
    "Load training data and explore auxiliary targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 511
    },
    "id": "R1I_xkY4xnNa",
    "outputId": "e65c1cff-8db6-4a21-8081-b256026f6f3a"
   },
   "outputs": [],
   "source": [
    "napi = NumerAPI()\n",
    "\n",
    "# Download metadata and training data\n",
    "print(\"Downloading metadata...\")\n",
    "napi.download_dataset(f\"{DATA_VERSION}/features.json\")\n",
    "print(\"Downloading training data...\")\n",
    "napi.download_dataset(f\"{DATA_VERSION}/train.parquet\")\n",
    "\n",
    "# Load feature metadata and define feature sets\n",
    "print(\"Loading feature metadata...\")\n",
    "feature_metadata = json.load(open(f\"{DATA_VERSION}/features.json\"))\n",
    "feature_sets = feature_metadata[\"feature_sets\"]\n",
    "original_feature_cols = feature_sets[FEATURE_SET_SIZE] # Use configured feature set size\n",
    "target_cols = feature_metadata[\"targets\"]\n",
    "\n",
    "# Load training data\n",
    "print(\"Loading training data...\")\n",
    "train = pd.read_parquet(\n",
    "    f\"{DATA_VERSION}/train.parquet\",\n",
    "    columns=[ERA_COL, DATA_TYPE_COL] + original_feature_cols + target_cols\n",
    ")\n",
    "\n",
    "# Filter for training data type (just in case)\n",
    "train = train[train[DATA_TYPE_COL] == \"train\"].copy()\n",
    "del train[DATA_TYPE_COL]\n",
    "gc.collect()\n",
    "\n",
    "# Downsample eras if configured\n",
    "if DOWNSAMPLE_TRAIN_ERAS > 1:\n",
    "    print(f\"Downsampling training data to every {DOWNSAMPLE_TRAIN_ERAS}th era...\")\n",
    "    train = train[train[ERA_COL].isin(train[ERA_COL].unique()[::DOWNSAMPLE_TRAIN_ERAS])].copy()\n",
    "    gc.collect()\n",
    "\n",
    "# Display target columns\n",
    "print(\"Target columns in training data:\")\n",
    "display(train[[ERA_COL] + target_cols].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4YzbRO5uxnNa"
   },
   "source": [
    "### The Main Target\n",
    "\n",
    "The primary target for Numerai predictions is typically `target_cyrusd_20`. The `target` column is often an alias for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pP6LnWcExnNa"
   },
   "outputs": [],
   "source": [
    "# Check if 'target' is an alias for the main target and prepare targets DataFrame\n",
    "if TARGET_COL in train.columns:\n",
    "    if not train[TARGET_COL].equals(train[MAIN_TARGET]):\n",
    "        warnings.warn(f\"'{TARGET_COL}' column is present but not equal to '{MAIN_TARGET}'. Check data consistency.\")\n",
    "    else:\n",
    "        print(f\"'{TARGET_COL}' column confirmed as alias for '{MAIN_TARGET}'.\")\n",
    "    # Keep only the main target and aux targets we need, drop the alias if it exists\n",
    "    targets_to_keep = [ERA_COL] + [MAIN_TARGET] + AUX_TARGETS\n",
    "    targets_df = train[[col for col in targets_to_keep if col in train.columns]].copy()\n",
    "    # Update target_cols list to reflect only those present and needed\n",
    "    target_cols = [col for col in [MAIN_TARGET] + AUX_TARGETS if col in targets_df.columns]\n",
    "else:\n",
    "    targets_df = train[[ERA_COL] + target_cols]\n",
    "\n",
    "print(f\"Using '{MAIN_TARGET}' as the main target.\")\n",
    "print(f\"Auxiliary targets being considered: {AUX_TARGETS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d46TQDtrxnNb"
   },
   "source": [
    "### Target Names and Correlations\n",
    "\n",
    "Auxiliary targets represent different stock market return definitions or time horizons (`_20` vs `_60` days). They have varying correlations with the main target, which can be useful for ensembling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 645
    },
    "id": "P7uAdarxxnNb",
    "outputId": "e0972e03-0e96-4b35-f304-0531c4592530"
   },
   "outputs": [],
   "source": [
    "# Print target names grouped by name and time horizon (using all available targets initially for mapping)\n",
    "all_target_cols = feature_metadata[\"targets\"]\n",
    "t20s = sorted([t for t in all_target_cols if t.endswith(\"_20\")])\n",
    "t60s = sorted([t for t in all_target_cols if t.endswith(\"_60\")])\n",
    "names = sorted(list(set([t.replace(\"target_\", \"\").replace(\"_20\", \"\").replace(\"_60\", \"\") for t in all_target_cols])))\n",
    "\n",
    "target_map_df = pd.DataFrame(index=names, columns=['20', '60'])\n",
    "for t in t20s:\n",
    "    name = t.replace(\"target_\", \"\").replace(\"_20\", \"\")\n",
    "    if name in target_map_df.index:\n",
    "      target_map_df.loc[name, '20'] = t\n",
    "for t in t60s:\n",
    "    name = t.replace(\"target_\", \"\").replace(\"_60\", \"\")\n",
    "    if name in target_map_df.index:\n",
    "      target_map_df.loc[name, '60'] = t\n",
    "\n",
    "print(\"Target names grouped by name and horizon:\")\n",
    "display(target_map_df.dropna(how='all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "target_corr_code"
   },
   "outputs": [],
   "source": [
    "# Calculate and display correlations with the main target (using only targets present in targets_df)\n",
    "print(f\"\\nCorrelations of available targets with {MAIN_TARGET}:\")\n",
    "if MAIN_TARGET in targets_df.columns:\n",
    "    target_corrs = (\n",
    "        targets_df[target_cols]\n",
    "        .corrwith(targets_df[MAIN_TARGET])\n",
    "        .sort_values(ascending=False)\n",
    "        .to_frame(f\"corr_with_{MAIN_TARGET}\")\n",
    "    )\n",
    "    display(target_corrs)\n",
    "\n",
    "    # Plot correlation matrix heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "      targets_df[target_cols].corr(),\n",
    "      cmap=\"coolwarm\",\n",
    "      xticklabels=False,\n",
    "      yticklabels=False\n",
    "    )\n",
    "    plt.title(\"Target Correlation Matrix\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"Main target {MAIN_TARGET} not found in the loaded training data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "apply_feature_eng_md"
   },
   "source": [
    "## 3. Apply Feature Engineering\n",
    "\n",
    "Generate UMAP, Denoising Autoencoder, Contrastive (placeholder), and CTGAN features. Fit on training data, transform validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "apply_feature_eng_code"
   },
   "outputs": [],
   "source": [
    "# --- Load Validation Data Before Feature Engineering Fit ---\n",
    "# This is necessary so we can apply the *fitted* transformers later\n",
    "print(\"\\nDownloading validation data...\")\n",
    "napi.download_dataset(f\"{DATA_VERSION}/validation.parquet\")\n",
    "print(\"Loading validation data...\")\n",
    "validation = pd.read_parquet(\n",
    "    f\"{DATA_VERSION}/validation.parquet\",\n",
    "    columns=[ERA_COL, DATA_TYPE_COL] + original_feature_cols + target_cols\n",
    ")\n",
    "validation = validation[validation[DATA_TYPE_COL] == \"validation\"].copy()\n",
    "del validation[DATA_TYPE_COL]\n",
    "gc.collect()\n",
    "\n",
    "# Downsample validation eras if configured\n",
    "if DOWNSAMPLE_VALID_ERAS > 1:\n",
    "    print(f\"Downsampling validation data to every {DOWNSAMPLE_VALID_ERAS}th era...\")\n",
    "    validation = validation[validation[ERA_COL].isin(validation[ERA_COL].unique()[::DOWNSAMPLE_VALID_ERAS])].copy()\n",
    "    gc.collect()\n",
    "\n",
    "# Embargo overlapping eras BEFORE feature engineering transform\n",
    "last_train_era = int(train[ERA_COL].astype(int).max())\n",
    "eras_to_embargo = [str(era).zfill(4) for era in range(last_train_era + 1, last_train_era + 5)] # Embargo 4 eras\n",
    "validation = validation[~validation[ERA_COL].isin(eras_to_embargo)].copy()\n",
    "print(f\"Embargoed eras from validation: {eras_to_embargo}\")\n",
    "gc.collect()\n",
    "\n",
    "# --- Fit Feature Engineering on Training Data & Transform Both ---\n",
    "engineered_feature_cols = []\n",
    "fitted_transformers = {} # Dictionary to store fitted objects\n",
    "\n",
    "# UMAP\n",
    "train, validation, umap_feats, fitted_transformers['umap_reducer'] = umap_feature_creation(\n",
    "    train, validation, original_feature_cols, n_components=UMAP_N_COMPONENTS\n",
    ")\n",
    "engineered_feature_cols.extend(umap_feats)\n",
    "gc.collect()\n",
    "\n",
    "# Denoising Autoencoder\n",
    "train, validation, ae_feats, fitted_transformers['ae_encoder'] = denoising_autoencoder_features(\n",
    "    train, validation, original_feature_cols, encoding_dim=AE_ENCODING_DIM, epochs=AE_EPOCHS\n",
    ")\n",
    "engineered_feature_cols.extend(ae_feats)\n",
    "gc.collect()\n",
    "\n",
    "# Contrastive Learning (Placeholder)\n",
    "train, validation, contrastive_feats, _ = contrastive_feature_creation(\n",
    "    train, validation, original_feature_cols, embedding_dim=CONTRASTIVE_EMB_DIM\n",
    ")\n",
    "engineered_feature_cols.extend(contrastive_feats)\n",
    "gc.collect()\n",
    "\n",
    "# CTGAN (using main target for demonstration)\n",
    "train, validation, ctgan_feats, fitted_transformers['ctgan_model'], fitted_transformers['qt'], fitted_transformers['synthetic_mean'] = synthetic_data_ctgan(\n",
    "    train, validation, original_feature_cols, MAIN_TARGET, epochs=CTGAN_EPOCHS\n",
    ")\n",
    "engineered_feature_cols.extend(ctgan_feats)\n",
    "gc.collect()\n",
    "\n",
    "# Update the main feature list\n",
    "feature_cols = original_feature_cols + engineered_feature_cols\n",
    "print(f\"\\nTotal number of features after engineering: {len(feature_cols)}\")\n",
    "print(f\"Engineered feature names: {engineered_feature_cols}\")\n",
    "\n",
    "print(\"\\nTraining data with engineered features (head):\")\n",
    "display(train[feature_cols].head())\n",
    "print(\"\\nValidation data with engineered features (head):\")\n",
    "display(validation[feature_cols].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "base_model_train_md"
   },
   "source": [
    "## 4. Base Model Training (LightGBM)\n",
    "\n",
    "Train LightGBM models for each selected target using the original and engineered features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VqBaYwmqxnNd",
    "outputId": "5da7bec6-e470-46a5-a553-b26e05a9084f"
   },
   "outputs": [],
   "source": [
    "print(\"Training LightGBM models on selected targets...\")\n",
    "models = {}\n",
    "for target in tqdm(TARGET_CANDIDATES, desc=\"Training models\"):\n",
    "    print(f\"Training model for {target}...\")\n",
    "    # Filter out rows where the current target is NaN for training\n",
    "    train_target_filtered = train.dropna(subset=[target])\n",
    "    \n",
    "    # Define LGBM parameters (consider adjusting based on feature set size)\n",
    "    lgbm_params = {\n",
    "        'n_estimators': 2000,\n",
    "        'learning_rate': 0.01,\n",
    "        'max_depth': 5,\n",
    "        'num_leaves': 2**4-1,\n",
    "        'colsample_bytree': 0.1,\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "    \n",
    "    model = lgb.LGBMRegressor(**lgbm_params)\n",
    "    model.fit(\n",
    "        train_target_filtered[feature_cols],\n",
    "        train_target_filtered[target]\n",
    "    )\n",
    "    models[target] = model\n",
    "    gc.collect()\n",
    "\n",
    "print(\"Base models trained.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "base_model_eval_md"
   },
   "source": [
    "### Base Model Evaluation\n",
    "\n",
    "Generate predictions on the validation set for each base model and evaluate their individual performance (Correlation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "base_model_eval_code"
   },
   "outputs": [],
   "source": [
    "from numerai_tools.scoring import numerai_corr\n",
    "\n",
    "print(\"Generating validation predictions for base models...\")\n",
    "validation_preds = pd.DataFrame(index=validation.index)\n",
    "for target_name, model in models.items():\n",
    "    pred_col_name = f\"prediction_{target_name}\"\n",
    "    # Ensure validation data has all features before predicting\n",
    "    validation_features = validation[feature_cols].fillna(0.5) # Handle potential NaNs introduced by FE on val\n",
    "    validation_preds[pred_col_name] = model.predict(validation_features)\n",
    "\n",
    "# Merge predictions back into the validation dataframe\n",
    "validation = validation.join(validation_preds)\n",
    "\n",
    "prediction_cols = list(validation_preds.columns)\n",
    "print(\"\\nValidation predictions generated:\")\n",
    "display(validation[prediction_cols].head())\n",
    "\n",
    "# Evaluate individual model correlations\n",
    "print(\"\\nEvaluating base model correlations...\")\n",
    "# Ensure MAIN_TARGET exists and has non-NA values for correlation calculation\n",
    "validation_eval = validation.dropna(subset=[MAIN_TARGET] + prediction_cols)\n",
    "if validation_eval.empty:\n",
    "    print(\"Warning: No valid rows remaining after dropping NaNs for correlation evaluation.\")\n",
    "    correlations = pd.DataFrame(columns=prediction_cols)\n",
    "    cumsum_corrs = pd.DataFrame(columns=prediction_cols)\n",
    "else:\n",
    "    correlations = validation_eval.groupby(ERA_COL).apply(\n",
    "        lambda d: numerai_corr(d[prediction_cols], d[MAIN_TARGET])\n",
    "    )\n",
    "    cumsum_corrs = correlations.cumsum()\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    cumsum_corrs.plot(ax=plt.gca())\n",
    "    plt.title(\"Cumulative Correlation of Base Model Validation Predictions\")\n",
    "    plt.xlabel(\"Era\")\n",
    "    plt.ylabel(\"Cumulative Correlation\")\n",
    "    plt.xticks([])\n",
    "    plt.legend(title=\"Model Target\")\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nSummary metrics for base models:\")\n",
    "def get_summary_metrics(scores, cumsum_scores):\n",
    "    summary_metrics = {}\n",
    "    mean = scores.mean()\n",
    "    std = scores.std()\n",
    "    sharpe = mean / std if std != 0 else np.nan\n",
    "    # Ensure cumsum_scores is not empty before calculating drawdown\n",
    "    if not cumsum_scores.empty:\n",
    "      rolling_max = cumsum_scores.expanding(min_periods=1).max()\n",
    "      max_drawdown = (rolling_max - cumsum_scores).max()\n",
    "    else:\n",
    "      max_drawdown = np.nan\n",
    "    return {\n",
    "        \"mean\": mean,\n",
    "        \"std\": std,\n",
    "        \"sharpe\": sharpe,\n",
    "        \"max_drawdown\": max_drawdown,\n",
    "    }\n",
    "\n",
    "base_model_summary = {}\n",
    "for pred_col in prediction_cols:\n",
    "    if pred_col in correlations.columns:\n",
    "      base_model_summary[pred_col] = get_summary_metrics(correlations[pred_col], cumsum_corrs[pred_col])\n",
    "    else:\n",
    "      base_model_summary[pred_col] = {'mean': np.nan, 'std': np.nan, 'sharpe': np.nan, 'max_drawdown': np.nan}\n",
    "\n",
    "summary_df = pd.DataFrame(base_model_summary).T\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "stacking_md"
   },
   "source": [
    "## 5. Stacked Ensembling\n",
    "\n",
    "Implement a stacked ensemble using out-of-fold predictions from the base models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "stacking_code"
   },
   "outputs": [],
   "source": [
    "print(\"Generating Out-of-Fold (OOF) predictions for stacking...\")\n",
    "\n",
    "gkf = GroupKFold(n_splits=N_FOLDS)\n",
    "oof_preds = pd.DataFrame(index=train.index)\n",
    "\n",
    "# Store OOF predictions for each base model\n",
    "for target_name, model in tqdm(models.items(), desc=\"Generating OOF preds\"):\n",
    "    print(f\" Generating OOF for {target_name}...\")\n",
    "    oof_preds_target = pd.Series(index=train.index, dtype=np.float32)\n",
    "    # Use only non-NaN target rows for training folds, but keep original index for alignment\n",
    "    train_target_filtered = train.dropna(subset=[target_name]) \n",
    "    \n",
    "    for fold, (train_idx_filtered, val_idx_filtered) in enumerate(gkf.split(train_target_filtered[feature_cols], train_target_filtered[target_name], groups=train_target_filtered[ERA_COL])):\n",
    "        # Map filtered indices back to original DataFrame indices\n",
    "        train_index_orig = train_target_filtered.iloc[train_idx_filtered].index\n",
    "        val_index_orig = train_target_filtered.iloc[val_idx_filtered].index\n",
    "\n",
    "        X_train_fold, X_val_fold = train.loc[train_index_orig, feature_cols], train.loc[val_index_orig, feature_cols]\n",
    "        y_train_fold = train.loc[train_index_orig, target_name]\n",
    "        \n",
    "        fold_model = lgb.LGBMRegressor(\n",
    "            n_estimators=500, # Fewer estimators for fold training\n",
    "            learning_rate=0.01,\n",
    "            max_depth=5,\n",
    "            num_leaves=2**4-1,\n",
    "            colsample_bytree=0.1,\n",
    "            random_state=fold, # Vary random state per fold\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        fold_model.fit(X_train_fold, y_train_fold)\n",
    "        \n",
    "        # Store predictions on the validation part of the fold, using original train index\n",
    "        oof_preds_target.loc[val_index_orig] = fold_model.predict(X_val_fold)\n",
    "        \n",
    "    oof_preds[f\"oof_{target_name}\"] = oof_preds_target\n",
    "    gc.collect()\n",
    "\n",
    "print(\"OOF predictions generated.\")\n",
    "display(oof_preds.head())\n",
    "\n",
    "# Prepare training data for the meta-model\n",
    "meta_train_features = oof_preds.copy()\n",
    "meta_train_features[ERA_COL] = train[ERA_COL] # Add era for potential use in meta-model\n",
    "meta_train_target = train[MAIN_TARGET]\n",
    "\n",
    "# Drop rows where OOF preds or target are NaN \n",
    "valid_indices = meta_train_target.notna() & meta_train_features.notna().all(axis=1)\n",
    "meta_train_features = meta_train_features.loc[valid_indices].copy()\n",
    "meta_train_target = meta_train_target.loc[valid_indices].copy()\n",
    "\n",
    "oof_feature_cols = list(oof_preds.columns)\n",
    "\n",
    "# Train the meta-model\n",
    "print(f\"\\nTraining meta-model ({STACKING_MODEL_TYPE})...\")\n",
    "if STACKING_MODEL_TYPE == 'LGBM':\n",
    "    meta_model = lgb.LGBMRegressor(\n",
    "        n_estimators=500,\n",
    "        learning_rate=0.01,\n",
    "        max_depth=3,\n",
    "        num_leaves=2**3-1,\n",
    "        colsample_bytree=0.8, # Use more features for meta-model\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    meta_model.fit(meta_train_features[oof_feature_cols], meta_train_target)\n",
    "elif STACKING_MODEL_TYPE == 'Linear':\n",
    "    scaler = StandardScaler() # Define scaler here\n",
    "    meta_train_features_scaled = scaler.fit_transform(meta_train_features[oof_feature_cols])\n",
    "    meta_model = Ridge(alpha=1.0, random_state=42)\n",
    "    meta_model.fit(meta_train_features_scaled, meta_train_target)\n",
    "    fitted_transformers['stacking_scaler'] = scaler # Store the scaler\n",
    "else:\n",
    "    raise ValueError(\"Invalid STACKING_MODEL_TYPE\")\n",
    "\n",
    "fitted_transformers['meta_model'] = meta_model # Store the meta-model\n",
    "print(\"Meta-model trained.\")\n",
    "\n",
    "# Generate predictions on the validation set using the stacking pipeline\n",
    "print(\"\\nGenerating stacked predictions on validation set...\")\n",
    "meta_val_features = validation[prediction_cols].copy()\n",
    "meta_val_features.columns = oof_feature_cols # Rename columns to match meta-model training\n",
    "\n",
    "# Handle potential NaNs in validation base predictions before meta-prediction\n",
    "meta_val_features = meta_val_features.fillna(meta_val_features.mean()) # Simple mean imputation\n",
    "\n",
    "if STACKING_MODEL_TYPE == 'Linear':\n",
    "    scaler_val = fitted_transformers['stacking_scaler'] # Use fitted scaler\n",
    "    meta_val_features_scaled = scaler_val.transform(meta_val_features)\n",
    "    stacked_preds = meta_model.predict(meta_val_features_scaled)\n",
    "else: # LGBM\n",
    "    stacked_preds = meta_model.predict(meta_val_features)\n",
    "\n",
    "validation[\"prediction_stacked\"] = stacked_preds\n",
    "\n",
    "print(\"Stacked predictions generated.\")\n",
    "display(validation[[\"prediction_stacked\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "stacking_eval_md"
   },
   "source": [
    "### Stacked Ensemble Evaluation\n",
    "\n",
    "Evaluate the performance of the stacked ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "stacking_eval_code"
   },
   "outputs": [],
   "source": [
    "print(\"Evaluating stacked ensemble performance...\")\n",
    "\n",
    "# Add stacked predictions to the list for evaluation\n",
    "evaluation_cols_stacking = prediction_cols + [\"prediction_stacked\"]\n",
    "\n",
    "# Ensure MAIN_TARGET exists and has non-NA values for correlation calculation\n",
    "validation_eval_stacking = validation.dropna(subset=[MAIN_TARGET] + evaluation_cols_stacking)\n",
    "\n",
    "if validation_eval_stacking.empty:\n",
    "    print(\"Warning: No valid rows remaining after dropping NaNs for stacking evaluation.\")\n",
    "    stacked_correlations = pd.DataFrame(columns=evaluation_cols_stacking)\n",
    "    stacked_cumsum_corrs = pd.DataFrame(columns=evaluation_cols_stacking)\n",
    "else:\n",
    "    stacked_correlations = validation_eval_stacking.groupby(ERA_COL).apply(\n",
    "        lambda d: numerai_corr(d[evaluation_cols_stacking], d[MAIN_TARGET])\n",
    "    )\n",
    "    stacked_cumsum_corrs = stacked_correlations.cumsum()\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    stacked_cumsum_corrs.plot(ax=plt.gca())\n",
    "    plt.title(\"Cumulative Correlation including Stacked Ensemble\")\n",
    "    plt.xlabel(\"Era\")\n",
    "    plt.ylabel(\"Cumulative Correlation\")\n",
    "    plt.xticks([])\n",
    "    plt.legend(title=\"Model\")\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nSummary metrics including Stacked Ensemble:\")\n",
    "stacked_summary = {}\n",
    "for pred_col in evaluation_cols_stacking:\n",
    "     if pred_col in stacked_correlations.columns:\n",
    "       stacked_summary[pred_col] = get_summary_metrics(stacked_correlations[pred_col], stacked_cumsum_corrs[pred_col])\n",
    "     else:\n",
    "       stacked_summary[pred_col] = {'mean': np.nan, 'std': np.nan, 'sharpe': np.nan, 'max_drawdown': np.nan}\n",
    "\n",
    "stacked_summary_df = pd.DataFrame(stacked_summary).T\n",
    "display(stacked_summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "era_invariant_md"
   },
   "source": [
    "## 6. Era-Invariant Training (PyTorch MLP Option)\n",
    "\n",
    "Define and train a PyTorch MLP with a custom loss function incorporating negative Pearson correlation, era correlation variance penalty, and feature exposure penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "era_invariant_code"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "# --- Define MLP Architecture ---\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.BatchNorm1d(input_dim), # Add BatchNorm\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid() # Output between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# --- Define Custom Loss Functions ---\n",
    "def pearson_corr(preds, target):\n",
    "    \"\"\"Calculate Pearson correlation coefficient (handles potential NaNs).\"\"\"\n",
    "    preds = preds.squeeze()\n",
    "    target = target.squeeze()\n",
    "    \n",
    "    preds_mean = torch.mean(preds)\n",
    "    target_mean = torch.mean(target)\n",
    "    \n",
    "    cov = torch.mean((preds - preds_mean) * (target - target_mean))\n",
    "    preds_std = torch.std(preds)\n",
    "    target_std = torch.std(target)\n",
    "    \n",
    "    epsilon = 1e-6\n",
    "    corr = cov / (preds_std * target_std + epsilon)\n",
    "    \n",
    "    # If correlation is NaN (e.g., due to zero std dev), return 0\n",
    "    return torch.nan_to_num(corr, nan=0.0)\n",
    "\n",
    "def era_correlation_variance_penalty(preds, target, eras):\n",
    "    \"\"\"Calculate variance of per-era correlations.\"\"\"\n",
    "    unique_eras = torch.unique(eras)\n",
    "    era_corrs = []\n",
    "    for era in unique_eras:\n",
    "        era_mask = (eras == era)\n",
    "        era_preds = preds[era_mask]\n",
    "        era_target = target[era_mask]\n",
    "        if len(era_preds) > 1: # Need at least 2 points for correlation\n",
    "            era_corrs.append(pearson_corr(era_preds, era_target))\n",
    "    \n",
    "    if len(era_corrs) > 1:\n",
    "        era_corrs_tensor = torch.stack(era_corrs)\n",
    "        # Filter out NaNs before calculating variance\n",
    "        valid_corrs = era_corrs_tensor[~torch.isnan(era_corrs_tensor)]\n",
    "        if len(valid_corrs) > 1:\n",
    "             return torch.var(valid_corrs)\n",
    "    return torch.tensor(0.0, device=preds.device)\n",
    "\n",
    "def feature_exposure_penalty(preds, features):\n",
    "    \"\"\"Calculate mean of squared correlations between predictions and features.\"\"\"\n",
    "    num_features = features.shape[1]\n",
    "    feature_corrs_sq = []\n",
    "    preds_squeezed = preds.squeeze()\n",
    "    \n",
    "    for i in range(num_features):\n",
    "        feature_col = features[:, i]\n",
    "        # Check if feature column has variance\n",
    "        if torch.std(feature_col) > 1e-6:\n",
    "             corr = pearson_corr(preds_squeezed, feature_col)\n",
    "             # Only append if correlation is not NaN\n",
    "             if not torch.isnan(corr):\n",
    "                 feature_corrs_sq.append(corr**2)\n",
    "        \n",
    "    if len(feature_corrs_sq) > 0:\n",
    "        return torch.mean(torch.stack(feature_corrs_sq))\n",
    "    return torch.tensor(0.0, device=preds.device)\n",
    "\n",
    "# --- Training Loop ---\n",
    "def train_mlp(train_df, feature_cols, target_col, era_col, original_feature_cols, top_n_features=TOP_N_FEATURES_FOR_EXPOSURE):\n",
    "    print(\"\\nTraining PyTorch MLP with custom loss...\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Prepare data\n",
    "    train_target_filtered = train_df.dropna(subset=[target_col])\n",
    "    features = torch.tensor(train_target_filtered[feature_cols].fillna(0.5).values, dtype=torch.float32).to(device)\n",
    "    target = torch.tensor(train_target_filtered[target_col].values, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    eras = torch.tensor(train_target_filtered[era_col].astype(int).values, dtype=torch.long).to(device)\n",
    "    \n",
    "    # Select top N original features for feature exposure penalty (based on correlation with target)\n",
    "    # Using original features for exposure penalty as requested\n",
    "    feature_corrs = train_target_filtered[original_feature_cols].corrwith(train_target_filtered[target_col])\n",
    "    top_feature_names = feature_corrs.abs().nlargest(top_n_features).index\n",
    "    top_features_tensor = torch.tensor(train_target_filtered[top_feature_names].fillna(0.5).values, dtype=torch.float32).to(device)\n",
    "    print(f\"Using top {len(top_feature_names)} original features for exposure penalty.\")\n",
    "\n",
    "    dataset = TensorDataset(features, target, eras, top_features_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=MLP_BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    # Initialize model, optimizer, loss weights\n",
    "    input_dim = len(feature_cols)\n",
    "    mlp_model = SimpleMLP(input_dim).to(device)\n",
    "    optimizer = optim.Adam(mlp_model.parameters(), lr=MLP_LR)\n",
    "    lambda1 = VARIANCE_PENALTY_WEIGHT\n",
    "    lambda2 = FEATURE_EXPOSURE_WEIGHT\n",
    "\n",
    "    # Training\n",
    "    for epoch in tqdm(range(MLP_EPOCHS), desc=\"Training MLP\"):\n",
    "        epoch_loss = 0.0\n",
    "        mlp_model.train()\n",
    "        for batch_features, batch_target, batch_eras, batch_top_features in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            preds = mlp_model(batch_features)\n",
    "\n",
    "            # Calculate loss components\n",
    "            corr_loss = -pearson_corr(preds, batch_target)\n",
    "            var_penalty = era_correlation_variance_penalty(preds, batch_target, batch_eras)\n",
    "            exposure_penalty = feature_exposure_penalty(preds, batch_top_features)\n",
    "\n",
    "            # Combine losses\n",
    "            total_loss = corr_loss + lambda1 * var_penalty + lambda2 * exposure_penalty\n",
    "            \n",
    "            if torch.isnan(total_loss):\n",
    "                # print(f\"Warning: NaN loss encountered in epoch {epoch+1}. Skipping batch.\")\n",
    "                # print(f\" corr: {corr_loss.item()}, var: {var_penalty.item()}, exp: {exposure_penalty.item()}\")\n",
    "                continue # Skip backpropagation if loss is NaN\n",
    "\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += total_loss.item()\n",
    "        \n",
    "        avg_epoch_loss = epoch_loss / len(dataloader) if len(dataloader) > 0 else 0\n",
    "        print(f\"Epoch [{epoch+1}/{MLP_EPOCHS}], Loss: {avg_epoch_loss:.6f}\")\n",
    "        \n",
    "    print(\"MLP training finished.\")\n",
    "    fitted_transformers['mlp_model'] = mlp_model.to('cpu') # Store the trained model\n",
    "    return mlp_model.to('cpu') # Move model back to CPU for prediction\n",
    "\n",
    "# --- Control Flow for Model Training ---\n",
    "mlp_model = None # Initialize to None\n",
    "if USE_MLP:\n",
    "    # Ensure PyTorch and dependencies are installed\n",
    "    try:\n",
    "        import torch\n",
    "        mlp_model = train_mlp(train, feature_cols, MAIN_TARGET, ERA_COL, original_feature_cols)\n",
    "        \n",
    "        # Generate MLP predictions on validation set\n",
    "        print(\"Generating MLP predictions on validation set...\")\n",
    "        mlp_model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Ensure validation data has all features and handle NaNs\n",
    "            val_features_tensor = torch.tensor(validation[feature_cols].fillna(0.5).values, dtype=torch.float32)\n",
    "            mlp_preds = mlp_model(val_features_tensor).numpy().squeeze()\n",
    "        validation[\"prediction_mlp\"] = mlp_preds\n",
    "        print(\"MLP predictions generated.\")\n",
    "        display(validation[[\"prediction_mlp\"]].head())\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"PyTorch not found. Skipping MLP training. Set USE_MLP=False or install PyTorch.\")\n",
    "        USE_MLP = False # Disable MLP usage if import fails\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during MLP training or prediction: {e}\")\n",
    "        mlp_model = None # Ensure mlp_model is None if training failed\n",
    "        USE_MLP = False # Disable MLP usage if training fails\n",
    "\n",
    "# Determine the final prediction column based on flags and success\n",
    "if USE_MLP and mlp_model:\n",
    "    print(\"Using MLP Ensemble based on configuration.\")\n",
    "    validation[\"prediction_final\"] = validation[\"prediction_mlp\"]\n",
    "elif USE_STACKING:\n",
    "    print(\"Using Stacked Ensemble based on configuration.\")\n",
    "    if \"prediction_stacked\" in validation.columns:\n",
    "        validation[\"prediction_final\"] = validation[\"prediction_stacked\"]\n",
    "    else:\n",
    "        print(\"Warning: Stacked predictions not found. Defaulting to main target model.\")\n",
    "        validation[\"prediction_final\"] = validation[f\"prediction_{MAIN_TARGET}\"]\n",
    "else:\n",
    "    print(\"Skipping Stacking and MLP training based on configuration.\")\n",
    "    print(\"Using the base model for the main target as the final prediction.\")\n",
    "    validation[\"prediction_final\"] = validation[f\"prediction_{MAIN_TARGET}\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "final_eval_md"
   },
   "source": [
    "## 7. Final Model Evaluation\n",
    "\n",
    "Evaluate the chosen final model (either Stacked Ensemble or MLP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "final_eval_code"
   },
   "outputs": [],
   "source": [
    "print(\"Evaluating final model performance...\")\n",
    "\n",
    "# Determine the final prediction column name again, ensuring it exists\n",
    "if USE_MLP and mlp_model and \"prediction_mlp\" in validation.columns:\n",
    "    final_pred_col = \"prediction_mlp\"\n",
    "    comparison_cols = [f\"prediction_{MAIN_TARGET}\", final_pred_col]\n",
    "elif USE_STACKING and \"prediction_stacked\" in validation.columns:\n",
    "    final_pred_col = \"prediction_stacked\"\n",
    "    comparison_cols = prediction_cols + [final_pred_col]\n",
    "else:\n",
    "    final_pred_col = f\"prediction_{MAIN_TARGET}\"\n",
    "    comparison_cols = [final_pred_col]\n",
    "    if final_pred_col not in validation.columns:\n",
    "         raise ValueError(\"Could not find a valid prediction column for final evaluation.\")\n",
    "\n",
    "print(f\"Evaluating column: {final_pred_col}\")\n",
    "\n",
    "# Ensure all columns for comparison exist in validation data\n",
    "existing_comparison_cols = [col for col in comparison_cols if col in validation.columns]\n",
    "if not existing_comparison_cols:\n",
    "     raise ValueError(\"No valid columns found for final comparison evaluation.\")\n",
    "\n",
    "# Ensure MAIN_TARGET exists and has non-NA values for correlation calculation\n",
    "validation_eval_final = validation.dropna(subset=[MAIN_TARGET] + existing_comparison_cols)\n",
    "\n",
    "if validation_eval_final.empty:\n",
    "    print(\"Warning: No valid rows remaining after dropping NaNs for final evaluation.\")\n",
    "    final_correlations = pd.DataFrame(columns=existing_comparison_cols)\n",
    "    final_cumsum_corrs = pd.DataFrame(columns=existing_comparison_cols)\n",
    "else:\n",
    "    final_correlations = validation_eval_final.groupby(ERA_COL).apply(\n",
    "        lambda d: numerai_corr(d[existing_comparison_cols], d[MAIN_TARGET])\n",
    "    )\n",
    "    final_cumsum_corrs = final_correlations.cumsum()\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    final_cumsum_corrs.plot(ax=plt.gca())\n",
    "    plt.title(f\"Cumulative Correlation of Final Model ({final_pred_col}) vs Others\")\n",
    "    plt.xlabel(\"Era\")\n",
    "    plt.ylabel(\"Cumulative Correlation\")\n",
    "    plt.xticks([])\n",
    "    plt.legend(title=\"Model\")\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nFinal Summary Metrics:\")\n",
    "final_summary = {}\n",
    "for pred_col in existing_comparison_cols:\n",
    "    if pred_col in final_correlations.columns:\n",
    "        final_summary[pred_col] = get_summary_metrics(final_correlations[pred_col], final_cumsum_corrs[pred_col])\n",
    "    else:\n",
    "        final_summary[pred_col] = {'mean': np.nan, 'std': np.nan, 'sharpe': np.nan, 'max_drawdown': np.nan}\n",
    "\n",
    "final_summary_df = pd.DataFrame(final_summary).T\n",
    "display(final_summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_upload_md"
   },
   "source": [
    "## 8. Model Upload\n",
    "\n",
    "Define the final prediction function based on the selected model (Stacking or MLP) and prepare for upload. This function now uses the *fitted* feature engineering objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WWdOTGy4xnNq"
   },
   "outputs": [],
   "source": [
    "# --- Define Final Prediction Function ---\n",
    "# This version assumes the necessary fitted objects are passed via the dependencies dictionary\n",
    "def predict_final(live_features: pd.DataFrame, dependencies: dict) -> pd.DataFrame:\n",
    "    \"\"\"Generates predictions using the chosen final model and pre-fitted FE objects.\"\"\"\n",
    "    \n",
    "    # Load dependencies\n",
    "    original_feature_cols = dependencies['original_feature_cols']\n",
    "    feature_cols = dependencies['feature_cols'] # Full list including engineered\n",
    "    umap_reducer = dependencies.get('umap_reducer')\n",
    "    ae_encoder = dependencies.get('ae_encoder')\n",
    "    qt = dependencies.get('qt') # QuantileTransformer for CTGAN\n",
    "    synthetic_mean = dependencies.get('synthetic_mean') # Mean for CTGAN distance\n",
    "    ae_feature_names = dependencies.get('ae_feats', [])\n",
    "    umap_feature_names = dependencies.get('umap_feats', [])\n",
    "    contrastive_feature_names = dependencies.get('contrastive_feats', [])\n",
    "    ctgan_feature_names = dependencies.get('ctgan_feats', [])\n",
    "    \n",
    "    models = dependencies['models'] # Base LGBM models\n",
    "    meta_model = dependencies.get('meta_model') # Stacking meta-model\n",
    "    scaler = dependencies.get('stacking_scaler') # Scaler for linear stacking\n",
    "    mlp_model_dep = dependencies.get('mlp_model') # PyTorch MLP model\n",
    "    \n",
    "    USE_MLP_FLAG = dependencies['USE_MLP']\n",
    "    USE_STACKING_FLAG = dependencies['USE_STACKING']\n",
    "    STACKING_MODEL_TYPE_FLAG = dependencies['STACKING_MODEL_TYPE']\n",
    "    MAIN_TARGET_NAME = dependencies['MAIN_TARGET']\n",
    "    PREDICTION_COL_NAME = dependencies['PREDICTION_COL']\n",
    "    \n",
    "    # Apply feature engineering transformations to live features\n",
    "    print(\"Applying feature engineering to live data...\")\n",
    "    live_features_eng = live_features.copy()\n",
    "    live_data_orig = live_features_eng[original_feature_cols].astype(np.float32).fillna(0.5)\n",
    "    \n",
    "    if umap_reducer and umap_feature_names:\n",
    "        print(\" Applying UMAP...\")\n",
    "        umap_features_live = umap_reducer.transform(live_data_orig)\n",
    "        live_features_eng[umap_feature_names] = umap_features_live\n",
    "        \n",
    "    if ae_encoder and ae_feature_names:\n",
    "        print(\" Applying AE...\")\n",
    "        ae_features_live = ae_encoder.predict(live_data_orig.values)\n",
    "        live_features_eng[ae_feature_names] = ae_features_live\n",
    "        \n",
    "    if contrastive_feature_names: # Placeholder: Generate random\n",
    "        print(\" Applying Contrastive (placeholder)...\")\n",
    "        num_samples_live = len(live_features_eng)\n",
    "        contrastive_features_live = np.random.rand(num_samples_live, len(contrastive_feature_names)).astype(np.float32)\n",
    "        live_features_eng[contrastive_feature_names] = contrastive_features_live\n",
    "\n",
    "    if ctgan_feature_names and synthetic_mean is not None:\n",
    "        print(\" Applying CTGAN distance feature...\")\n",
    "        distances_live = np.linalg.norm(live_data_orig.values - synthetic_mean.values, axis=1)\n",
    "        live_features_eng[ctgan_feature_names[0]] = distances_live\n",
    "    \n",
    "    print(\"Feature engineering applied to live data.\")\n",
    "\n",
    "    # Ensure all feature columns are present, filling missing ones (e.g., if FE failed)\n",
    "    for col in feature_cols:\n",
    "        if col not in live_features_eng.columns:\n",
    "            print(f\"Warning: Feature column '{col}' missing in live data. Filling with 0.5.\")\n",
    "            live_features_eng[col] = 0.5 \n",
    "            \n",
    "    live_features_eng = live_features_eng[feature_cols].fillna(0.5) # Final check for NaNs\n",
    "\n",
    "    # --- Prediction Logic ---\n",
    "    if USE_MLP_FLAG and mlp_model_dep:\n",
    "        print(\"Generating predictions using MLP model...\")\n",
    "        mlp_model_dep.eval()\n",
    "        with torch.no_grad():\n",
    "            live_features_tensor = torch.tensor(live_features_eng.values, dtype=torch.float32)\n",
    "            predictions = mlp_model_dep(live_features_tensor).numpy().squeeze()\n",
    "        submission_df = pd.DataFrame({'prediction': predictions}, index=live_features.index)\n",
    "        \n",
    "    elif USE_STACKING_FLAG and meta_model:\n",
    "        print(\"Generating predictions using Stacked Ensemble...\")\n",
    "        base_preds_live = pd.DataFrame(index=live_features.index)\n",
    "        oof_cols = [f\"oof_{t}\" for t in models.keys()]\n",
    "        for target_name, model in models.items():\n",
    "            base_preds_live[f\"oof_{target_name}\"] = model.predict(live_features_eng)\n",
    "        \n",
    "        base_preds_live = base_preds_live.fillna(base_preds_live.mean()) # Impute NaNs if any base pred failed\n",
    "        \n",
    "        if STACKING_MODEL_TYPE_FLAG == 'Linear' and scaler:\n",
    "             base_preds_live_scaled = scaler.transform(base_preds_live[oof_cols])\n",
    "             stacked_preds_live = meta_model.predict(base_preds_live_scaled)\n",
    "        else: # LGBM\n",
    "             stacked_preds_live = meta_model.predict(base_preds_live[oof_cols])\n",
    "        \n",
    "        submission_df = pd.DataFrame({'prediction': stacked_preds_live}, index=live_features.index)\n",
    "\n",
    "    else: # Default to main target base model\n",
    "        print(f\"Generating predictions using base model for {MAIN_TARGET_NAME}...\")\n",
    "        predictions = models[MAIN_TARGET_NAME].predict(live_features_eng)\n",
    "        submission_df = pd.DataFrame({'prediction': predictions}, index=live_features.index)\n",
    "\n",
    "    # Rank predictions for submission\n",
    "    ranked_submission = submission_df['prediction'].rank(pct=True, method=\"first\")\n",
    "    return ranked_submission.to_frame(PREDICTION_COL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 473
    },
    "id": "kPq_ATf0xnNr",
    "outputId": "dc1b5ed0-e3d6-4d19-ba95-58d3a7c5f44b"
   },
   "outputs": [],
   "source": [
    "# --- Quick Test on Live Data ---\n",
    "print(\"Downloading live features for testing...\")\n",
    "napi.download_dataset(f\"{DATA_VERSION}/live.parquet\")\n",
    "live_features = pd.read_parquet(f\"{DATA_VERSION}/live.parquet\", columns=original_feature_cols)\n",
    "\n",
    "# Prepare dependencies dictionary for the test\n",
    "test_dependencies = {\n",
    "    'models': models,\n",
    "    'umap_reducer': fitted_transformers.get('umap_reducer'),\n",
    "    'ae_encoder': fitted_transformers.get('ae_encoder'),\n",
    "    'qt': fitted_transformers.get('qt'),\n",
    "    'synthetic_mean': fitted_transformers.get('synthetic_mean'),\n",
    "    'meta_model': fitted_transformers.get('meta_model'),\n",
    "    'stacking_scaler': fitted_transformers.get('stacking_scaler'),\n",
    "    'mlp_model': fitted_transformers.get('mlp_model'),\n",
    "    'original_feature_cols': original_feature_cols,\n",
    "    'engineered_feature_cols': engineered_feature_cols,\n",
    "    'feature_cols': feature_cols,\n",
    "    'ae_feats': ae_feats,\n",
    "    'umap_feats': umap_feats,\n",
    "    'contrastive_feats': contrastive_feats,\n",
    "    'ctgan_feats': ctgan_feats,\n",
    "    'UMAP_N_COMPONENTS': UMAP_N_COMPONENTS,\n",
    "    'AE_ENCODING_DIM': AE_ENCODING_DIM,\n",
    "    'CONTRASTIVE_EMB_DIM': CONTRASTIVE_EMB_DIM,\n",
    "    'USE_MLP': USE_MLP,\n",
    "    'USE_STACKING': USE_STACKING,\n",
    "    'STACKING_MODEL_TYPE': STACKING_MODEL_TYPE,\n",
    "    'MAIN_TARGET': MAIN_TARGET,\n",
    "    'PREDICTION_COL': PREDICTION_COL\n",
    "}\n",
    "\n",
    "# Generate predictions using the final function\n",
    "final_predictions = predict_final(live_features, test_dependencies)\n",
    "\n",
    "print(\"\\nSample of final predictions:\")\n",
    "display(final_predictions.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pickle_upload_code"
   },
   "outputs": [],
   "source": [
    "# --- Pickle the Prediction Function and Dependencies ---\n",
    "print(\"Pickling the prediction function...\")\n",
    "try:\n",
    "    # Define the dictionary containing the function and its necessary dependencies\n",
    "    pickle_payload = {\n",
    "        'predict_fn': predict_final,\n",
    "        'dependencies': {\n",
    "            'models': models, # Base LGBM models\n",
    "            'umap_reducer': fitted_transformers.get('umap_reducer'),\n",
    "            'ae_encoder': fitted_transformers.get('ae_encoder'),\n",
    "            'qt': fitted_transformers.get('qt'), # QuantileTransformer for CTGAN\n",
    "            'synthetic_mean': fitted_transformers.get('synthetic_mean'), # Mean for CTGAN distance\n",
    "            'meta_model': fitted_transformers.get('meta_model'), # Stacking meta-model\n",
    "            'stacking_scaler': fitted_transformers.get('stacking_scaler'), # Scaler for linear stacking\n",
    "            'mlp_model': fitted_transformers.get('mlp_model'), # PyTorch MLP model\n",
    "            'original_feature_cols': original_feature_cols,\n",
    "            'engineered_feature_cols': engineered_feature_cols,\n",
    "            'feature_cols': feature_cols,\n",
    "            'ae_feats': ae_feats, # List of AE feature names\n",
    "            'umap_feats': umap_feats, # List of UMAP feature names\n",
    "            'contrastive_feats': contrastive_feats, # List of Contrastive feature names\n",
    "            'ctgan_feats': ctgan_feats, # List of CTGAN feature names\n",
    "            'UMAP_N_COMPONENTS': UMAP_N_COMPONENTS,\n",
    "            'AE_ENCODING_DIM': AE_ENCODING_DIM,\n",
    "            'CONTRASTIVE_EMB_DIM': CONTRASTIVE_EMB_DIM,\n",
    "            'USE_MLP': USE_MLP,\n",
    "            'USE_STACKING': USE_STACKING,\n",
    "            'STACKING_MODEL_TYPE': STACKING_MODEL_TYPE,\n",
    "            'MAIN_TARGET': MAIN_TARGET,\n",
    "            'PREDICTION_COL': PREDICTION_COL\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Register libraries that cloudpickle might struggle with by default\n",
    "    cloudpickle.register_pickle_by_value(umap)\n",
    "    cloudpickle.register_pickle_by_value(tf)\n",
    "    cloudpickle.register_pickle_by_value(torch)\n",
    "    cloudpickle.register_pickle_by_value(ctgan)\n",
    "    \n",
    "    # Pickle the payload\n",
    "    with open(\"predict_final_model.pkl\", \"wb\") as f:\n",
    "        cloudpickle.dump(pickle_payload, f)\n",
    "    print(\"Prediction function and dependencies pickled successfully to predict_final_model.pkl\")\n",
    "\n",
    "except NameError as e:\n",
    "    print(f\"Pickling failed: A required object might not be defined. Error: {e}\")\n",
    "    print(\"Ensure all models and transformers used in 'predict_final' are trained and available globally or passed correctly.\")\n",
    "except Exception as e:\n",
    "     print(f\"An unexpected error occurred during pickling: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "download_final_pkl"
   },
   "outputs": [],
   "source": [
    "# Download file if running in Google Colab\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download('predict_final_model.pkl')\n",
    "except ImportError:\n",
    "    print(\"Skipping download (not in Colab environment).\")\n",
    "except Exception as e:\n",
    "    print(f\"File download failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jxO2YzmIxnNr"
   },
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "This notebook demonstrated adding feature engineering, stacked ensembling, and an optional era-invariant MLP training pipeline to the original target ensembling notebook.\n",
    "\n",
    "Remember to choose the model (Stacking or MLP) you want to submit by setting the `USE_STACKING` or `USE_MLP` flags before pickling and uploading `predict_final_model.pkl` to [numer.ai](https://numer.ai)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
