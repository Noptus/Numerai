{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FOUvgVp3xnNW"
   },
   "source": [
    "# Numerai Modeling: Feature Engineering, Ensembling, and Advanced Training (with Checkpoints)\n",
    "\n",
    "This notebook demonstrates several advanced modeling techniques for the Numerai tournament, broken down into sections with checkpoints to manage resource usage, optimized for lower RAM environments (~12GB).\n",
    "1. **Setup & Configuration**\n",
    "2. **Part 1: Data Loading & Initial Exploration** (Checkpoint: Saves raw train/validation data with optimized types)\n",
    "3. **Part 2: Feature Engineering** (Checkpoint: Saves data with engineered features and fitted transformers/column lists)\n",
    "4. **Part 3: Base Model Training** (Checkpoint: Saves trained base models)\n",
    "5. **Part 4: Stacked Ensembling** (Checkpoint: Saves OOF predictions and meta-model)\n",
    "6. **Part 5: Era-Invariant MLP Training (Optional)** (Checkpoint: Saves trained MLP model)\n",
    "7. **Part 6: Final Evaluation & Prediction Function**\n",
    "8. **Part 7: Model Pickling for Upload**\n",
    "\n",
    "**Workflow:** Run each part sequentially. If you stop and restart, the notebook will attempt to load data from the last completed checkpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_config_md"
   },
   "source": [
    "## Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KD826S8uxnNY"
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q numerapi pandas pyarrow matplotlib lightgbm scikit-learn cloudpickle==2.2.1 seaborn umap-learn tensorflow torch ctgan tqdm\n",
    "\n",
    "# Inline plots\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "config_vars"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import gc\n",
    "import os\n",
    "import tensorflow as tf # Import TF early to potentially suppress warnings\n",
    "from numerapi import NumerAPI\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer\n",
    "import cloudpickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "\n",
    "# Ignore specific warnings if needed (e.g., from CTGAN or TF)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='sklearn.preprocessing._data')\n",
    "tf.get_logger().setLevel('ERROR') # Suppress TensorFlow warnings\n",
    "\n",
    "# --- Configuration ---\n",
    "DATA_VERSION = \"v5.0\"\n",
    "MAIN_TARGET = \"target_cyrusd_20\"\n",
    "AUX_TARGETS = [\n",
    "  \"target_victor_20\",\n",
    "  \"target_xerxes_20\",\n",
    "  \"target_teager2b_20\"\n",
    "]\n",
    "TARGET_CANDIDATES = [MAIN_TARGET] + AUX_TARGETS\n",
    "ERA_COL = \"era\"\n",
    "DATA_TYPE_COL = \"data_type\"\n",
    "TARGET_COL = \"target\" # Alias for MAIN_TARGET in original notebook\n",
    "PREDICTION_COL = \"prediction\"\n",
    "ID_COL = \"id\" # Define id column name\n",
    "\n",
    "# Feature Engineering Hyperparameters (Adjusted for lower RAM)\n",
    "UMAP_N_COMPONENTS = 30 # Reduced UMAP components\n",
    "AE_ENCODING_DIM = 32   # Reduced AE dimensions\n",
    "CONTRASTIVE_EMB_DIM = 32 # Reduced Contrastive dimensions\n",
    "CTGAN_EPOCHS = 20      # Significantly reduced CTGAN epochs\n",
    "AE_EPOCHS = 3          # Reduced AE epochs\n",
    "CTGAN_SYNTH_RATIO = 0.2 # Generate fewer synthetic samples\n",
    "\n",
    "# Stacking Ensemble Config\n",
    "N_FOLDS = 5 # Number of folds for OOF predictions\n",
    "STACKING_MODEL_TYPE = 'LGBM' # 'LGBM' or 'Linear'\n",
    "\n",
    "# PyTorch MLP Config (Adjusted for lower RAM)\n",
    "MLP_EPOCHS = 3          # Reduced MLP epochs\n",
    "MLP_BATCH_SIZE = 2048   # Increased batch size can sometimes help RAM if GPU is used, but monitor\n",
    "MLP_LR = 0.001\n",
    "VARIANCE_PENALTY_WEIGHT = 0.01 # lambda1\n",
    "FEATURE_EXPOSURE_WEIGHT = 0.01 # lambda2\n",
    "TOP_N_FEATURES_FOR_EXPOSURE = 30 # Reduced features for exposure penalty\n",
    "\n",
    "# Model Selection Flags\n",
    "USE_STACKING = True # Set to True to use Stacking, False for MLP\n",
    "USE_MLP = False      # Set to True to use MLP (requires PyTorch)\n",
    "\n",
    "# Speedup Options (Highly Recommended for faster iteration)\n",
    "DOWNSAMPLE_TRAIN_ERAS = 4 # Use every Nth era for training (e.g., 4 or 10)\n",
    "DOWNSAMPLE_VALID_ERAS = 4 # Use every Nth era for validation (e.g., 4 or 10)\n",
    "FEATURE_SET_SIZE = \"small\" # 'small', 'medium', or 'all'\n",
    "\n",
    "# Checkpoint File Paths\n",
    "CHECKPOINT_DIR = \"checkpoints\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "CP1_TRAIN_PATH = os.path.join(CHECKPOINT_DIR, \"train_part1.parquet\")\n",
    "CP1_VALID_PATH = os.path.join(CHECKPOINT_DIR, \"validation_part1.parquet\")\n",
    "CP2_TRAIN_PATH = os.path.join(CHECKPOINT_DIR, \"train_part2.parquet\")\n",
    "CP2_VALID_PATH = os.path.join(CHECKPOINT_DIR, \"validation_part2.parquet\")\n",
    "CP2_FE_INFO_PATH = os.path.join(CHECKPOINT_DIR, \"fe_info_part2.pkl\") # Changed name\n",
    "CP3_MODELS_PATH = os.path.join(CHECKPOINT_DIR, \"base_models_part3.pkl\")\n",
    "CP4_OOF_PATH = os.path.join(CHECKPOINT_DIR, \"oof_preds_part4.parquet\")\n",
    "CP4_META_MODEL_PATH = os.path.join(CHECKPOINT_DIR, \"meta_model_part4.pkl\")\n",
    "CP5_MLP_MODEL_PATH = os.path.join(CHECKPOINT_DIR, \"mlp_model_part5.pkl\")\n",
    "\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', lambda x: f'{x:.6f}')\n",
    "\n",
    "# Add comment about GPU potential\n",
    "# For significant speedups, especially with AE, CTGAN, and MLP, ensure you are running\n",
    "# in an environment with a GPU and have the necessary GPU versions of \n",
    "# TensorFlow and PyTorch installed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "part1_md"
   },
   "source": [
    "## Part 1: Data Loading & Initial Exploration\n",
    "\n",
    "Load training and validation data, perform initial filtering, downsampling, and **data type optimization**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "part1_code"
   },
   "outputs": [],
   "source": [
    "napi = NumerAPI()\n",
    "\n",
    "# Function to optimize data types\n",
    "def optimize_dtypes(df, feature_cols, target_cols):\n",
    "    print(\"Optimizing data types...\")\n",
    "    for col in df.columns:\n",
    "        if col in feature_cols:\n",
    "            df[col] = df[col].astype(np.int8) # Numerai features are 0-4\n",
    "        elif col in target_cols:\n",
    "            df[col] = df[col].astype(np.float16) # Targets are 0, 0.25, ..., 1\n",
    "        elif col == ERA_COL:\n",
    "             df[col] = df[col].astype(np.int16) # Eras are integers\n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "# Check if checkpoint exists\n",
    "if os.path.exists(CP1_TRAIN_PATH) and os.path.exists(CP1_VALID_PATH):\n",
    "    print(f\"Loading data from Checkpoint 1...\")\n",
    "    train = pd.read_parquet(CP1_TRAIN_PATH).set_index(ID_COL)\n",
    "    validation = pd.read_parquet(CP1_VALID_PATH).set_index(ID_COL)\n",
    "    # Load metadata to reconstruct column lists\n",
    "    if not os.path.exists(f\"{DATA_VERSION}/features.json\"):\n",
    "        print(\"Downloading metadata...\")\n",
    "        napi.download_dataset(f\"{DATA_VERSION}/features.json\")\n",
    "    feature_metadata = json.load(open(f\"{DATA_VERSION}/features.json\"))\n",
    "    feature_sets = feature_metadata[\"feature_sets\"]\n",
    "    original_feature_cols = feature_sets[FEATURE_SET_SIZE]\n",
    "    target_cols_meta = feature_metadata[\"targets\"]\n",
    "    # Recreate targets_df if needed for exploration (or save/load it too)\n",
    "    target_cols = [col for col in [MAIN_TARGET] + AUX_TARGETS if col in train.columns]\n",
    "    targets_to_keep = [ERA_COL] + target_cols\n",
    "    targets_df = train[[col for col in targets_to_keep if col in train.columns]].copy()\n",
    "    print(\"Data loaded from Checkpoint 1.\")\n",
    "else:\n",
    "    print(\"Checkpoint 1 not found. Loading data from source...\")\n",
    "    # Download metadata and training data\n",
    "    print(\"Downloading metadata...\")\n",
    "    napi.download_dataset(f\"{DATA_VERSION}/features.json\")\n",
    "    print(\"Downloading training data...\")\n",
    "    napi.download_dataset(f\"{DATA_VERSION}/train.parquet\")\n",
    "    print(\"Downloading validation data...\")\n",
    "    napi.download_dataset(f\"{DATA_VERSION}/validation.parquet\")\n",
    "\n",
    "    # Load feature metadata and define feature sets\n",
    "    print(\"Loading feature metadata...\")\n",
    "    feature_metadata = json.load(open(f\"{DATA_VERSION}/features.json\"))\n",
    "    feature_sets = feature_metadata[\"feature_sets\"]\n",
    "    original_feature_cols = feature_sets[FEATURE_SET_SIZE]\n",
    "    target_cols_meta = feature_metadata[\"targets\"]\n",
    "\n",
    "    # Define columns to read (including 'id')\n",
    "    read_columns = [ID_COL, ERA_COL, DATA_TYPE_COL] + original_feature_cols + target_cols_meta\n",
    "\n",
    "    # Load training data - read specified columns first\n",
    "    print(\"Loading training data...\")\n",
    "    train_raw = pd.read_parquet(f\"{DATA_VERSION}/train.parquet\", columns=read_columns)\n",
    "    train = train_raw[train_raw[DATA_TYPE_COL] == \"train\"].set_index(ID_COL).copy()\n",
    "    del train[DATA_TYPE_COL]\n",
    "    del train_raw\n",
    "    gc.collect()\n",
    "\n",
    "    # Load validation data - read specified columns first\n",
    "    print(\"Loading validation data...\")\n",
    "    validation_raw = pd.read_parquet(f\"{DATA_VERSION}/validation.parquet\", columns=read_columns)\n",
    "    validation = validation_raw[validation_raw[DATA_TYPE_COL] == \"validation\"].set_index(ID_COL).copy()\n",
    "    del validation[DATA_TYPE_COL]\n",
    "    del validation_raw\n",
    "    gc.collect()\n",
    "\n",
    "    # --- Initial Preprocessing & Downsampling ---\n",
    "    # Check if 'target' is an alias and update target_cols\n",
    "    if TARGET_COL in train.columns:\n",
    "        if not train[TARGET_COL].equals(train[MAIN_TARGET]):\n",
    "            warnings.warn(f\"'{TARGET_COL}' column is present but not equal to '{MAIN_TARGET}'. Check data consistency.\")\n",
    "        else:\n",
    "            print(f\"'{TARGET_COL}' column confirmed as alias for '{MAIN_TARGET}'.\")\n",
    "        target_cols = [col for col in target_cols_meta if col != TARGET_COL] # Use original meta list\n",
    "        train = train.drop(columns=[TARGET_COL], errors='ignore')\n",
    "        validation = validation.drop(columns=[TARGET_COL], errors='ignore')\n",
    "    else:\n",
    "        target_cols = target_cols_meta\n",
    "\n",
    "    target_cols = [col for col in [MAIN_TARGET] + AUX_TARGETS if col in train.columns] # Ensure only needed targets remain\n",
    "\n",
    "    # Optimize Dtypes *before* downsampling/embargo if possible, otherwise after\n",
    "    train = optimize_dtypes(train, original_feature_cols, target_cols)\n",
    "    validation = optimize_dtypes(validation, original_feature_cols, target_cols)\n",
    "\n",
    "    # Downsample eras if configured\n",
    "    if DOWNSAMPLE_TRAIN_ERAS > 1:\n",
    "        print(f\"Downsampling training data to every {DOWNSAMPLE_TRAIN_ERAS}th era...\")\n",
    "        train = train[train[ERA_COL].isin(train[ERA_COL].unique()[::DOWNSAMPLE_TRAIN_ERAS])].copy()\n",
    "        gc.collect()\n",
    "    if DOWNSAMPLE_VALID_ERAS > 1:\n",
    "        print(f\"Downsampling validation data to every {DOWNSAMPLE_VALID_ERAS}th era...\")\n",
    "        validation = validation[validation[ERA_COL].isin(validation[ERA_COL].unique()[::DOWNSAMPLE_VALID_ERAS])].copy()\n",
    "        gc.collect()\n",
    "\n",
    "    # Embargo validation eras\n",
    "    last_train_era = int(train[ERA_COL].astype(int).max())\n",
    "    eras_to_embargo = [str(era).zfill(4) for era in range(last_train_era + 1, last_train_era + 5)]\n",
    "    validation = validation[~validation[ERA_COL].isin(eras_to_embargo)].copy()\n",
    "    print(f\"Embargoed eras from validation: {eras_to_embargo}\")\n",
    "    gc.collect()\n",
    "\n",
    "    # Save Checkpoint 1\n",
    "    print(\"Saving Checkpoint 1...\")\n",
    "    train.reset_index().to_parquet(CP1_TRAIN_PATH)\n",
    "    validation.reset_index().to_parquet(CP1_VALID_PATH)\n",
    "    print(\"Checkpoint 1 saved.\")\n",
    "    \n",
    "    # Recreate targets_df for exploration after potential drops/downsampling\n",
    "    targets_to_keep = [ERA_COL] + target_cols\n",
    "    targets_df = train[[col for col in targets_to_keep if col in train.columns]].copy()\n",
    "\n",
    "# --- Exploration Code (Runs whether loading from checkpoint or source) ---\n",
    "print(f\"Using '{MAIN_TARGET}' as the main target.\")\n",
    "print(f\"Auxiliary targets being considered: {AUX_TARGETS}\")\n",
    "\n",
    "# Display target columns\n",
    "print(\"Target columns in training data (head):\")\n",
    "display(targets_df.head())\n",
    "\n",
    "# Display target correlations\n",
    "print(f\"\\nCorrelations of available targets with {MAIN_TARGET}:\")\n",
    "if MAIN_TARGET in targets_df.columns:\n",
    "    target_corrs = (\n",
    "        targets_df[target_cols]\n",
    "        .corrwith(targets_df[MAIN_TARGET].astype(float)) # Ensure float for correlation\n",
    "        .sort_values(ascending=False)\n",
    "        .to_frame(f\"corr_with_{MAIN_TARGET}\")\n",
    "    )\n",
    "    display(target_corrs)\n",
    "\n",
    "    # Plot correlation matrix heatmap\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "      targets_df[target_cols].astype(float).corr(), # Ensure float for correlation matrix\n",
    "      cmap=\"coolwarm\",\n",
    "      xticklabels=False,\n",
    "      yticklabels=False\n",
    "    )\n",
    "    plt.title(\"Target Correlation Matrix\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"Main target {MAIN_TARGET} not found in the loaded training data.\")\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "part2_md"
   },
   "source": [
    "## Part 2: Feature Engineering\n",
    "\n",
    "Fit feature engineering models/transformers on training data and apply transformations to both train and validation sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "feature_eng_funcs_md_moved"
   },
   "source": [
    "### Feature Engineering Functions Definition\n",
    "(Moved function definitions here for clarity in the checkpoint structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "feature_eng_funcs_code_moved"
   },
   "outputs": [],
   "source": [
    "import umap\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from ctgan import CTGAN\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "# --- UMAP Feature Creation ---\n",
    "def umap_feature_creation(df_train, df_transform, feature_cols, n_components=UMAP_N_COMPONENTS, random_state=42):\n",
    "    \"\"\"Fits UMAP on df_train and transforms both df_train and df_transform.\"\"\"\n",
    "    print(f\"Creating {n_components} UMAP features...\")\n",
    "    reducer = umap.UMAP(n_components=n_components, random_state=random_state, n_jobs=1)\n",
    "    print(\" Fitting UMAP on training data...\")\n",
    "    # Use float32 for UMAP input\n",
    "    train_data = df_train[feature_cols].astype(np.float32).fillna(0.5)\n",
    "    reducer.fit(train_data)\n",
    "    print(\" Transforming training data...\")\n",
    "    umap_features_train = reducer.transform(train_data).astype(np.float32) # Output as float32\n",
    "    umap_feature_names = [f\"umap_feat_{i}\" for i in range(n_components)]\n",
    "    df_train[umap_feature_names] = umap_features_train\n",
    "    print(\" Transforming second dataframe...\")\n",
    "    transform_data = df_transform[feature_cols].astype(np.float32).fillna(0.5)\n",
    "    umap_features_transform = reducer.transform(transform_data).astype(np.float32)\n",
    "    df_transform[umap_feature_names] = umap_features_transform\n",
    "    print(\"UMAP features created and applied.\")\n",
    "    return df_train, df_transform, umap_feature_names, reducer\n",
    "\n",
    "# --- Denoising Autoencoder Feature Creation ---\n",
    "def denoising_autoencoder_features(df_train, df_transform, feature_cols, encoding_dim=AE_ENCODING_DIM, noise_factor=0.1, epochs=AE_EPOCHS, batch_size=1024):\n",
    "    \"\"\"Fits AE on df_train and transforms both df_train and df_transform.\"\"\"\n",
    "    print(f\"Creating {encoding_dim} Denoising AE features...\")\n",
    "    input_dim = len(feature_cols)\n",
    "    # Use float32 for TF input\n",
    "    train_data = df_train[feature_cols].astype(np.float32).fillna(0.5).values\n",
    "    noisy_train_data = train_data + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=train_data.shape)\n",
    "    noisy_train_data = np.clip(noisy_train_data, 0., 1.).astype(np.float32)\n",
    "    train_data = train_data.astype(np.float32)\n",
    "\n",
    "    input_layer = keras.Input(shape=(input_dim,))\n",
    "    encoded = layers.Dense(encoding_dim * 2, activation='relu')(input_layer)\n",
    "    encoded = layers.Dense(encoding_dim, activation='relu', name='encoder_output')(encoded)\n",
    "    decoded = layers.Dense(encoding_dim * 2, activation='relu')(encoded)\n",
    "    decoded = layers.Dense(input_dim, activation='sigmoid')(decoded)\n",
    "    autoencoder = keras.Model(input_layer, decoded)\n",
    "    encoder = keras.Model(input_layer, encoded)\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    print(\" Training Denoising Autoencoder...\")\n",
    "    autoencoder.fit(noisy_train_data, train_data, epochs=epochs, batch_size=batch_size, shuffle=True, validation_split=0.1, verbose=0)\n",
    "    print(\" Encoding training data...\")\n",
    "    ae_features_train = encoder.predict(train_data).astype(np.float32) # Output as float32\n",
    "    ae_feature_names = [f\"ae_feat_{i}\" for i in range(encoding_dim)]\n",
    "    df_train[ae_feature_names] = ae_features_train\n",
    "    print(\" Encoding second dataframe...\")\n",
    "    transform_data = df_transform[feature_cols].astype(np.float32).fillna(0.5).values\n",
    "    ae_features_transform = encoder.predict(transform_data).astype(np.float32)\n",
    "    df_transform[ae_feature_names] = ae_features_transform\n",
    "    print(\"AE features created and applied.\")\n",
    "    return df_train, df_transform, ae_feature_names, encoder\n",
    "\n",
    "# --- Contrastive Learning Feature Creation (Placeholder) ---\n",
    "def contrastive_feature_creation(df_train, df_transform, feature_cols, embedding_dim=CONTRASTIVE_EMB_DIM):\n",
    "    \"\"\"Placeholder: Generates random features for both dataframes.\"\"\"\n",
    "    print(f\"Creating {embedding_dim} Contrastive (placeholder) features...\")\n",
    "    num_samples_train = len(df_train)\n",
    "    num_samples_transform = len(df_transform)\n",
    "    contrastive_features_train = np.random.rand(num_samples_train, embedding_dim).astype(np.float32)\n",
    "    contrastive_features_transform = np.random.rand(num_samples_transform, embedding_dim).astype(np.float32)\n",
    "    contrastive_feature_names = [f\"contrastive_feat_{i}\" for i in range(embedding_dim)]\n",
    "    df_train[contrastive_feature_names] = contrastive_features_train\n",
    "    df_transform[contrastive_feature_names] = contrastive_features_transform\n",
    "    print(\"Contrastive (placeholder) features created.\")\n",
    "    return df_train, df_transform, contrastive_feature_names, None \n",
    "\n",
    "# --- CTGAN Feature Creation ---\n",
    "def synthetic_data_ctgan(df_train, df_transform, feature_cols, target_col, n_synthetic_samples_ratio=CTGAN_SYNTH_RATIO, epochs=CTGAN_EPOCHS):\n",
    "    \"\"\"Fits CTGAN on df_train and generates a distance feature for both dataframes.\"\"\"\n",
    "    print(f\"Creating synthetic features using CTGAN (target: {target_col})...\")\n",
    "    # Use only a subset for CTGAN fitting if memory is tight\n",
    "    train_subset_for_ctgan = df_train.sample(frac=0.5, random_state=42) # Use 50% for fitting\n",
    "    data_subset_train = train_subset_for_ctgan[feature_cols + [target_col]].copy().dropna(subset=[target_col])\n",
    "    data_subset_train[feature_cols] = data_subset_train[feature_cols].fillna(0.5)\n",
    "    qt = QuantileTransformer(output_distribution='uniform', random_state=42)\n",
    "    print(\" Fitting QuantileTransformer...\")\n",
    "    # Fit QT only on the subset used for CTGAN\n",
    "    data_transformed_train = qt.fit_transform(data_subset_train[feature_cols])\n",
    "    data_transformed_df_train = pd.DataFrame(data_transformed_train, columns=feature_cols, index=data_subset_train.index)\n",
    "    data_transformed_df_train[target_col] = data_subset_train[target_col].values\n",
    "    discrete_columns = []\n",
    "    print(\" Training CTGAN...\")\n",
    "    ctgan_model = CTGAN(verbose=False)\n",
    "    try:\n",
    "        ctgan_model.fit(data_transformed_df_train, discrete_columns, epochs=epochs)\n",
    "    except Exception as e:\n",
    "        print(f\"CTGAN fitting failed: {e}. Skipping CTGAN features.\")\n",
    "        return df_train, df_transform, [], None, None, None\n",
    "    print(\" Generating synthetic data...\")\n",
    "    n_synthetic_samples = int(len(data_subset_train) * n_synthetic_samples_ratio)\n",
    "    if n_synthetic_samples == 0:\n",
    "        print(\"Warning: n_synthetic_samples_ratio is too low, generating 0 samples. Skipping CTGAN features.\")\n",
    "        return df_train, df_transform, [], None, None, None\n",
    "    synthetic_data_transformed = ctgan_model.sample(n_synthetic_samples)\n",
    "    synthetic_features_original_scale = qt.inverse_transform(synthetic_data_transformed[feature_cols])\n",
    "    synthetic_df = pd.DataFrame(synthetic_features_original_scale, columns=feature_cols)\n",
    "    synthetic_mean = synthetic_df.mean(axis=0).astype(np.float32)\n",
    "    ctgan_feature_name = f\"dist_to_synth_mean_{target_col}\"\n",
    "    print(\" Calculating distance feature for training data...\")\n",
    "    original_features_train = df_train[feature_cols].fillna(0.5).values.astype(np.float32)\n",
    "    distances_train = np.linalg.norm(original_features_train - synthetic_mean.values, axis=1).astype(np.float32)\n",
    "    df_train[ctgan_feature_name] = distances_train\n",
    "    print(\" Calculating distance feature for second dataframe...\")\n",
    "    original_features_transform = df_transform[feature_cols].fillna(0.5).values.astype(np.float32)\n",
    "    distances_transform = np.linalg.norm(original_features_transform - synthetic_mean.values, axis=1).astype(np.float32)\n",
    "    df_transform[ctgan_feature_name] = distances_transform\n",
    "    print(\"CTGAN-derived features created and applied.\")\n",
    "    del data_subset_train, data_transformed_train, data_transformed_df_train, synthetic_data_transformed, synthetic_features_original_scale, synthetic_df\n",
    "    gc.collect()\n",
    "    return df_train, df_transform, [ctgan_feature_name], ctgan_model, qt, synthetic_mean\n",
    "\n",
    "print(\"Feature engineering functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "part2_code"
   },
   "outputs": [],
   "source": [
    "# Check if Checkpoint 2 exists\n",
    "if os.path.exists(CP2_TRAIN_PATH) and os.path.exists(CP2_VALID_PATH) and os.path.exists(CP2_FE_INFO_PATH):\n",
    "    print(\"Loading data from Checkpoint 2...\")\n",
    "    train = pd.read_parquet(CP2_TRAIN_PATH).set_index(ID_COL)\n",
    "    validation = pd.read_parquet(CP2_VALID_PATH).set_index(ID_COL)\n",
    "    with open(CP2_FE_INFO_PATH, 'rb') as f:\n",
    "        fe_info = cloudpickle.load(f)\n",
    "        fitted_transformers = fe_info['transformers']\n",
    "        original_feature_cols = fe_info['original_feature_cols']\n",
    "        engineered_feature_cols = fe_info['engineered_feature_cols']\n",
    "        feature_cols = fe_info['feature_cols']\n",
    "        target_cols = fe_info['target_cols'] # Load target cols too\n",
    "    \n",
    "    # Ensure loaded dataframes have the correct columns (handle potential schema drift)\n",
    "    required_cols = [ERA_COL] + target_cols + feature_cols\n",
    "    train = train[[col for col in required_cols if col in train.columns]]\n",
    "    validation = validation[[col for col in required_cols if col in validation.columns]]\n",
    "    \n",
    "    print(f\"Data, {len(fitted_transformers)} fitted transformers, and feature lists loaded from Checkpoint 2.\")\n",
    "    print(f\"Total features: {len(feature_cols)}\")\n",
    "\n",
    "else:\n",
    "    print(\"Checkpoint 2 not found. Running Feature Engineering...\")\n",
    "    # --- Fit Feature Engineering on Training Data & Transform Both ---\n",
    "    engineered_feature_cols = []\n",
    "    fitted_transformers = {} # Dictionary to store fitted objects\n",
    "\n",
    "    # UMAP\n",
    "    train, validation, umap_feats, fitted_transformers['umap_reducer'] = umap_feature_creation(\n",
    "        train, validation, original_feature_cols, n_components=UMAP_N_COMPONENTS\n",
    "    )\n",
    "    engineered_feature_cols.extend(umap_feats)\n",
    "    gc.collect()\n",
    "\n",
    "    # Denoising Autoencoder\n",
    "    train, validation, ae_feats, fitted_transformers['ae_encoder'] = denoising_autoencoder_features(\n",
    "        train, validation, original_feature_cols, encoding_dim=AE_ENCODING_DIM, epochs=AE_EPOCHS\n",
    "    )\n",
    "    engineered_feature_cols.extend(ae_feats)\n",
    "    gc.collect()\n",
    "\n",
    "    # Contrastive Learning (Placeholder)\n",
    "    train, validation, contrastive_feats, _ = contrastive_feature_creation(\n",
    "        train, validation, original_feature_cols, embedding_dim=CONTRASTIVE_EMB_DIM\n",
    "    )\n",
    "    engineered_feature_cols.extend(contrastive_feats)\n",
    "    gc.collect()\n",
    "\n",
    "    # CTGAN (using main target for demonstration)\n",
    "    # WARNING: CTGAN can be very memory intensive. If this step fails,\n",
    "    # consider reducing CTGAN_SYNTH_RATIO or commenting out this block.\n",
    "    print(\"\\n--- Starting CTGAN --- (This may take time and memory)\")\n",
    "    train, validation, ctgan_feats, fitted_transformers['ctgan_model'], fitted_transformers['qt'], fitted_transformers['synthetic_mean'] = synthetic_data_ctgan(\n",
    "        train, validation, original_feature_cols, MAIN_TARGET, epochs=CTGAN_EPOCHS, n_synthetic_samples_ratio=CTGAN_SYNTH_RATIO\n",
    "    )\n",
    "    engineered_feature_cols.extend(ctgan_feats)\n",
    "    print(\"--- Finished CTGAN ---\")\n",
    "    gc.collect()\n",
    "\n",
    "    # Update the main feature list\n",
    "    feature_cols = original_feature_cols + engineered_feature_cols\n",
    "    print(f\"\\nTotal number of features after engineering: {len(feature_cols)}\")\n",
    "\n",
    "    # Optimize dtypes for engineered features (mostly floats)\n",
    "    for df in [train, validation]:\n",
    "        for col in engineered_feature_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].astype(np.float32)\n",
    "    gc.collect()\n",
    "\n",
    "    # Save Checkpoint 2\n",
    "    print(\"Saving Checkpoint 2...\")\n",
    "    train.reset_index().to_parquet(CP2_TRAIN_PATH)\n",
    "    validation.reset_index().to_parquet(CP2_VALID_PATH)\n",
    "    \n",
    "    # Save fitted transformers and feature lists\n",
    "    fe_info_to_save = {\n",
    "        'transformers': fitted_transformers,\n",
    "        'original_feature_cols': original_feature_cols,\n",
    "        'engineered_feature_cols': engineered_feature_cols,\n",
    "        'feature_cols': feature_cols,\n",
    "        'target_cols': target_cols\n",
    "    }\n",
    "    try:\n",
    "        with open(CP2_FE_INFO_PATH, 'wb') as f:\n",
    "            cloudpickle.dump(fe_info_to_save, f)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not pickle FE info: {e}. Some FE steps might need re-running.\")\n",
    "        if os.path.exists(CP2_FE_INFO_PATH):\n",
    "            os.remove(CP2_FE_INFO_PATH)\n",
    "    print(\"Checkpoint 2 saved.\")\n",
    "\n",
    "# Display head of dataframes after FE\n",
    "print(\"\\nTraining data with engineered features (head):\")\n",
    "display(train[feature_cols].head())\n",
    "print(\"\\nValidation data with engineered features (head):\")\n",
    "display(validation[feature_cols].head())\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "part3_md"
   },
   "source": [
    "## Part 3: Base Model Training (LightGBM)\n",
    "\n",
    "Train LightGBM models for each selected target using the original and engineered features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "part3_code"
   },
   "outputs": [],
   "source": [
    "# Check if Checkpoint 3 exists\n",
    "if os.path.exists(CP3_MODELS_PATH):\n",
    "    print(\"Loading base models from Checkpoint 3...\")\n",
    "    with open(CP3_MODELS_PATH, 'rb') as f:\n",
    "        models = cloudpickle.load(f)\n",
    "    print(f\"{len(models)} base models loaded.\")\n",
    "else:\n",
    "    print(\"Checkpoint 3 not found. Training LightGBM models...\")\n",
    "    models = {}\n",
    "    for target in tqdm(TARGET_CANDIDATES, desc=\"Training base models\"):\n",
    "        print(f\"Training model for {target}...\")\n",
    "        train_target_filtered = train.dropna(subset=[target])\n",
    "        \n",
    "        lgbm_params = {\n",
    "            'n_estimators': 2000,\n",
    "            'learning_rate': 0.01,\n",
    "            'max_depth': 5,\n",
    "            'num_leaves': 2**4-1,\n",
    "            'colsample_bytree': 0.1,\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "        \n",
    "        model = lgb.LGBMRegressor(**lgbm_params)\n",
    "        # Ensure feature columns exist before fitting\n",
    "        train_features = train_target_filtered[[col for col in feature_cols if col in train_target_filtered.columns]].fillna(0.5)\n",
    "        model.fit(train_features, train_target_filtered[target].astype(np.float32)) # Ensure target is float32\n",
    "        models[target] = model\n",
    "        gc.collect()\n",
    "\n",
    "    # Save Checkpoint 3\n",
    "    print(\"Saving Checkpoint 3...\")\n",
    "    with open(CP3_MODELS_PATH, 'wb') as f:\n",
    "        cloudpickle.dump(models, f)\n",
    "    print(\"Checkpoint 3 saved.\")\n",
    "\n",
    "# --- Base Model Evaluation (always run after loading/training) ---\n",
    "print(\"\\nGenerating validation predictions for base models...\")\n",
    "validation_preds = pd.DataFrame(index=validation.index)\n",
    "for target_name, model in models.items():\n",
    "    pred_col_name = f\"prediction_{target_name}\"\n",
    "    validation_features = validation[feature_cols].fillna(0.5)\n",
    "    validation_preds[pred_col_name] = model.predict(validation_features)\n",
    "\n",
    "# Join predictions, handling potential duplicate column names if cell is re-run\n",
    "validation = validation.drop(columns=validation_preds.columns, errors='ignore').join(validation_preds)\n",
    "prediction_cols = list(validation_preds.columns)\n",
    "\n",
    "print(\"\\nValidation predictions generated:\")\n",
    "display(validation[prediction_cols].head())\n",
    "\n",
    "print(\"\\nEvaluating base model correlations...\")\n",
    "validation_eval_base = validation.dropna(subset=[MAIN_TARGET] + prediction_cols)\n",
    "if validation_eval_base.empty:\n",
    "    print(\"Warning: No valid rows for base model correlation evaluation.\")\n",
    "    correlations = pd.DataFrame(columns=prediction_cols)\n",
    "    cumsum_corrs = pd.DataFrame(columns=prediction_cols)\n",
    "else:\n",
    "    correlations = validation_eval_base.groupby(ERA_COL).apply(\n",
    "        lambda d: numerai_corr(d[prediction_cols], d[MAIN_TARGET])\n",
    "    )\n",
    "    cumsum_corrs = correlations.cumsum()\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    cumsum_corrs.plot(ax=plt.gca())\n",
    "    plt.title(\"Cumulative Correlation of Base Model Validation Predictions\")\n",
    "    plt.xlabel(\"Era\")\n",
    "    plt.ylabel(\"Cumulative Correlation\")\n",
    "    plt.xticks([])\n",
    "    plt.legend(title=\"Model Target\")\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nSummary metrics for base models:\")\n",
    "def get_summary_metrics(scores, cumsum_scores):\n",
    "    summary_metrics = {}\n",
    "    mean = scores.mean()\n",
    "    std = scores.std()\n",
    "    sharpe = mean / std if std != 0 else np.nan\n",
    "    if not cumsum_scores.empty:\n",
    "      rolling_max = cumsum_scores.expanding(min_periods=1).max()\n",
    "      max_drawdown = (rolling_max - cumsum_scores).max()\n",
    "    else:\n",
    "      max_drawdown = np.nan\n",
    "    return {\n",
    "        \"mean\": mean,\n",
    "        \"std\": std,\n",
    "        \"sharpe\": sharpe,\n",
    "        \"max_drawdown\": max_drawdown,\n",
    "    }\n",
    "\n",
    "base_model_summary = {}\n",
    "for pred_col in prediction_cols:\n",
    "    if pred_col in correlations.columns:\n",
    "        base_model_summary[pred_col] = get_summary_metrics(correlations[pred_col], cumsum_corrs[pred_col])\n",
    "    else:\n",
    "        base_model_summary[pred_col] = {'mean': np.nan, 'std': np.nan, 'sharpe': np.nan, 'max_drawdown': np.nan}\n",
    "\n",
    "summary_df_base = pd.DataFrame(base_model_summary).T\n",
    "display(summary_df_base)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "part4_md"
   },
   "source": [
    "## Part 4: Stacked Ensembling\n",
    "\n",
    "Generate OOF predictions and train the meta-model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "part4_code"
   },
   "outputs": [],
   "source": [
    "meta_model = None\n",
    "scaler = None\n",
    "oof_preds = None\n",
    "\n",
    "# Check if Checkpoint 4 exists\n",
    "if USE_STACKING and os.path.exists(CP4_OOF_PATH) and os.path.exists(CP4_META_MODEL_PATH):\n",
    "    print(\"Loading OOF predictions and meta-model from Checkpoint 4...\")\n",
    "    oof_preds = pd.read_parquet(CP4_OOF_PATH).set_index(ID_COL)\n",
    "    with open(CP4_META_MODEL_PATH, 'rb') as f:\n",
    "        meta_model_data = cloudpickle.load(f)\n",
    "        meta_model = meta_model_data['meta_model']\n",
    "        if STACKING_MODEL_TYPE == 'Linear':\n",
    "            scaler = meta_model_data.get('scaler') # Load scaler if it exists\n",
    "            fitted_transformers['stacking_scaler'] = scaler\n",
    "    fitted_transformers['meta_model'] = meta_model\n",
    "    print(\"OOF predictions and meta-model loaded.\")\n",
    "\n",
    "elif USE_STACKING:\n",
    "    print(\"Checkpoint 4 not found. Generating OOF predictions and training meta-model...\")\n",
    "    gkf = GroupKFold(n_splits=N_FOLDS)\n",
    "    oof_preds = pd.DataFrame(index=train.index)\n",
    "\n",
    "    for target_name, model in tqdm(models.items(), desc=\"Generating OOF preds\"):\n",
    "        print(f\" Generating OOF for {target_name}...\")\n",
    "        oof_preds_target = pd.Series(index=train.index, dtype=np.float32)\n",
    "        train_target_filtered = train.dropna(subset=[target_name])\n",
    "        \n",
    "        for fold, (train_idx_filtered, val_idx_filtered) in enumerate(gkf.split(train_target_filtered[feature_cols], train_target_filtered[target_name], groups=train_target_filtered[ERA_COL])):\n",
    "            train_index_orig = train_target_filtered.iloc[train_idx_filtered].index\n",
    "            val_index_orig = train_target_filtered.iloc[val_idx_filtered].index\n",
    "            X_train_fold, X_val_fold = train.loc[train_index_orig, feature_cols], train.loc[val_index_orig, feature_cols]\n",
    "            y_train_fold = train.loc[train_index_orig, target_name]\n",
    "            fold_model = lgb.LGBMRegressor(n_estimators=500, learning_rate=0.01, max_depth=5, num_leaves=2**4-1, colsample_bytree=0.1, random_state=fold, n_jobs=-1)\n",
    "            fold_model.fit(X_train_fold, y_train_fold)\n",
    "            oof_preds_target.loc[val_index_orig] = fold_model.predict(X_val_fold)\n",
    "            \n",
    "        oof_preds[f\"oof_{target_name}\"] = oof_preds_target\n",
    "        gc.collect()\n",
    "    \n",
    "    print(\"OOF predictions generated.\")\n",
    "    display(oof_preds.head())\n",
    "\n",
    "    # Prepare training data for the meta-model\n",
    "    meta_train_features = oof_preds.copy()\n",
    "    meta_train_features[ERA_COL] = train[ERA_COL]\n",
    "    meta_train_target = train[MAIN_TARGET]\n",
    "    valid_indices = meta_train_target.notna() & meta_train_features.notna().all(axis=1)\n",
    "    meta_train_features = meta_train_features.loc[valid_indices].copy()\n",
    "    meta_train_target = meta_train_target.loc[valid_indices].copy()\n",
    "    oof_feature_cols = list(oof_preds.columns)\n",
    "\n",
    "    # Train the meta-model\n",
    "    print(f\"\\nTraining meta-model ({STACKING_MODEL_TYPE})...\")\n",
    "    meta_model_to_save = {}\n",
    "    if STACKING_MODEL_TYPE == 'LGBM':\n",
    "        meta_model = lgb.LGBMRegressor(n_estimators=500, learning_rate=0.01, max_depth=3, num_leaves=2**3-1, colsample_bytree=0.8, random_state=42, n_jobs=-1)\n",
    "        meta_model.fit(meta_train_features[oof_feature_cols], meta_train_target.astype(np.float32))\n",
    "        meta_model_to_save['meta_model'] = meta_model\n",
    "    elif STACKING_MODEL_TYPE == 'Linear':\n",
    "        scaler = StandardScaler()\n",
    "        meta_train_features_scaled = scaler.fit_transform(meta_train_features[oof_feature_cols])\n",
    "        meta_model = Ridge(alpha=1.0, random_state=42)\n",
    "        meta_model.fit(meta_train_features_scaled, meta_train_target.astype(np.float32))\n",
    "        meta_model_to_save['meta_model'] = meta_model\n",
    "        meta_model_to_save['scaler'] = scaler\n",
    "        fitted_transformers['stacking_scaler'] = scaler\n",
    "    else:\n",
    "        raise ValueError(\"Invalid STACKING_MODEL_TYPE\")\n",
    "    \n",
    "    fitted_transformers['meta_model'] = meta_model\n",
    "    print(\"Meta-model trained.\")\n",
    "\n",
    "    # Save Checkpoint 4\n",
    "    print(\"Saving Checkpoint 4...\")\n",
    "    oof_preds.reset_index().to_parquet(CP4_OOF_PATH)\n",
    "    with open(CP4_META_MODEL_PATH, 'wb') as f:\n",
    "        cloudpickle.dump(meta_model_to_save, f)\n",
    "    print(\"Checkpoint 4 saved.\")\n",
    "    gc.collect()\n",
    "else:\n",
    "    print(\"Skipping Stacking Ensemble based on configuration (USE_STACKING=False).\")\n",
    "\n",
    "# --- Stacked Ensemble Evaluation (if run) ---\n",
    "if USE_STACKING and meta_model is not None:\n",
    "    print(\"\\nGenerating stacked predictions on validation set for evaluation...\")\n",
    "    oof_feature_cols = [f\"oof_{t}\" for t in TARGET_CANDIDATES] # Define OOF columns based on candidates\n",
    "    meta_val_features = validation[[f\"prediction_{t}\" for t in TARGET_CANDIDATES]].copy()\n",
    "    meta_val_features.columns = oof_feature_cols # Rename columns\n",
    "    meta_val_features = meta_val_features.fillna(meta_val_features.mean())\n",
    "\n",
    "    if STACKING_MODEL_TYPE == 'Linear':\n",
    "        scaler_val = fitted_transformers['stacking_scaler']\n",
    "        meta_val_features_scaled = scaler_val.transform(meta_val_features)\n",
    "        stacked_preds = meta_model.predict(meta_val_features_scaled)\n",
    "    else: # LGBM\n",
    "        stacked_preds = meta_model.predict(meta_val_features)\n",
    "\n",
    "    validation[\"prediction_stacked\"] = stacked_preds\n",
    "    print(\"Stacked predictions generated for validation.\")\n",
    "    display(validation[[\"prediction_stacked\"]].head())\n",
    "\n",
    "    print(\"\\nEvaluating stacked ensemble performance...\")\n",
    "    evaluation_cols_stacking = prediction_cols + [\"prediction_stacked\"]\n",
    "    validation_eval_stacking = validation.dropna(subset=[MAIN_TARGET] + evaluation_cols_stacking)\n",
    "\n",
    "    if validation_eval_stacking.empty:\n",
    "        print(\"Warning: No valid rows for stacking evaluation.\")\n",
    "    else:\n",
    "        stacked_correlations = validation_eval_stacking.groupby(ERA_COL).apply(\n",
    "            lambda d: numerai_corr(d[evaluation_cols_stacking], d[MAIN_TARGET])\n",
    "        )\n",
    "        stacked_cumsum_corrs = stacked_correlations.cumsum()\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        stacked_cumsum_corrs.plot(ax=plt.gca())\n",
    "        plt.title(\"Cumulative Correlation including Stacked Ensemble\")\n",
    "        plt.xlabel(\"Era\")\n",
    "        plt.ylabel(\"Cumulative Correlation\")\n",
    "        plt.xticks([])\n",
    "        plt.legend(title=\"Model\")\n",
    "        plt.grid(True, linestyle='--', alpha=0.5)\n",
    "        plt.show()\n",
    "\n",
    "        print(\"\\nSummary metrics including Stacked Ensemble:\")\n",
    "        stacked_summary = {}\n",
    "        for pred_col in evaluation_cols_stacking:\n",
    "             if pred_col in stacked_correlations.columns:\n",
    "               stacked_summary[pred_col] = get_summary_metrics(stacked_correlations[pred_col], stacked_cumsum_corrs[pred_col])\n",
    "             else:\n",
    "               stacked_summary[pred_col] = {'mean': np.nan, 'std': np.nan, 'sharpe': np.nan, 'max_drawdown': np.nan}\n",
    "        stacked_summary_df = pd.DataFrame(stacked_summary).T\n",
    "        display(stacked_summary_df)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "part5_md"
   },
   "source": [
    "## Part 5: Era-Invariant Training (PyTorch MLP Option)\n",
    "\n",
    "Define and train a PyTorch MLP with custom loss functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "era_invariant_funcs_md"
   },
   "source": [
    "### MLP and Custom Loss Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "era_invariant_funcs_code"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "# --- Define MLP Architecture ---\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.BatchNorm1d(input_dim), \n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# --- Define Custom Loss Functions ---\n",
    "def pearson_corr(preds, target):\n",
    "    preds = preds.squeeze()\n",
    "    target = target.squeeze()\n",
    "    preds_mean = torch.mean(preds)\n",
    "    target_mean = torch.mean(target)\n",
    "    cov = torch.mean((preds - preds_mean) * (target - target_mean))\n",
    "    preds_std = torch.std(preds)\n",
    "    target_std = torch.std(target)\n",
    "    epsilon = 1e-6\n",
    "    corr = cov / (preds_std * target_std + epsilon)\n",
    "    return torch.nan_to_num(corr, nan=0.0)\n",
    "\n",
    "def era_correlation_variance_penalty(preds, target, eras):\n",
    "    unique_eras = torch.unique(eras)\n",
    "    era_corrs = []\n",
    "    for era in unique_eras:\n",
    "        era_mask = (eras == era)\n",
    "        era_preds = preds[era_mask]\n",
    "        era_target = target[era_mask]\n",
    "        if len(era_preds) > 1:\n",
    "            era_corrs.append(pearson_corr(era_preds, era_target))\n",
    "    if len(era_corrs) > 1:\n",
    "        era_corrs_tensor = torch.stack(era_corrs)\n",
    "        valid_corrs = era_corrs_tensor[~torch.isnan(era_corrs_tensor)]\n",
    "        if len(valid_corrs) > 1:\n",
    "             return torch.var(valid_corrs)\n",
    "    return torch.tensor(0.0, device=preds.device)\n",
    "\n",
    "def feature_exposure_penalty(preds, features):\n",
    "    num_features = features.shape[1]\n",
    "    feature_corrs_sq = []\n",
    "    preds_squeezed = preds.squeeze()\n",
    "    for i in range(num_features):\n",
    "        feature_col = features[:, i]\n",
    "        if torch.std(feature_col) > 1e-6:\n",
    "             corr = pearson_corr(preds_squeezed, feature_col)\n",
    "             if not torch.isnan(corr):\n",
    "                 feature_corrs_sq.append(corr**2)\n",
    "    if len(feature_corrs_sq) > 0:\n",
    "        return torch.mean(torch.stack(feature_corrs_sq))\n",
    "    return torch.tensor(0.0, device=preds.device)\n",
    "\n",
    "# --- Training Loop Function ---\n",
    "def train_mlp(train_df, feature_cols, target_col, era_col, original_feature_cols, top_n_features=TOP_N_FEATURES_FOR_EXPOSURE):\n",
    "    print(\"\\nTraining PyTorch MLP with custom loss...\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    train_target_filtered = train_df.dropna(subset=[target_col])\n",
    "    features = torch.tensor(train_target_filtered[feature_cols].fillna(0.5).values, dtype=torch.float32).to(device)\n",
    "    target = torch.tensor(train_target_filtered[target_col].values, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    eras = torch.tensor(train_target_filtered[era_col].astype(int).values, dtype=torch.long).to(device)\n",
    "    feature_corrs = train_target_filtered[original_feature_cols].corrwith(train_target_filtered[target_col].astype(float))\n",
    "    top_feature_names = feature_corrs.abs().nlargest(top_n_features).index\n",
    "    top_features_tensor = torch.tensor(train_target_filtered[top_feature_names].fillna(0.5).values, dtype=torch.float32).to(device)\n",
    "    print(f\"Using top {len(top_feature_names)} original features for exposure penalty.\")\n",
    "    dataset = TensorDataset(features, target, eras, top_features_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=MLP_BATCH_SIZE, shuffle=True)\n",
    "    input_dim = len(feature_cols)\n",
    "    mlp_model_local = SimpleMLP(input_dim).to(device)\n",
    "    optimizer = optim.Adam(mlp_model_local.parameters(), lr=MLP_LR)\n",
    "    lambda1 = VARIANCE_PENALTY_WEIGHT\n",
    "    lambda2 = FEATURE_EXPOSURE_WEIGHT\n",
    "    for epoch in tqdm(range(MLP_EPOCHS), desc=\"Training MLP\"):\n",
    "        epoch_loss = 0.0\n",
    "        mlp_model_local.train()\n",
    "        for batch_features, batch_target, batch_eras, batch_top_features in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            preds = mlp_model_local(batch_features)\n",
    "            corr_loss = -pearson_corr(preds, batch_target)\n",
    "            var_penalty = era_correlation_variance_penalty(preds, batch_target, batch_eras)\n",
    "            exposure_penalty = feature_exposure_penalty(preds, batch_top_features)\n",
    "            total_loss = corr_loss + lambda1 * var_penalty + lambda2 * exposure_penalty\n",
    "            if torch.isnan(total_loss):\n",
    "                continue\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += total_loss.item()\n",
    "        avg_epoch_loss = epoch_loss / len(dataloader) if len(dataloader) > 0 else 0\n",
    "        print(f\"Epoch [{epoch+1}/{MLP_EPOCHS}], Loss: {avg_epoch_loss:.6f}\")\n",
    "    print(\"MLP training finished.\")\n",
    "    return mlp_model_local.to('cpu')\n",
    "\n",
    "print(\"MLP and custom loss functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "part5_code"
   },
   "outputs": [],
   "source": [
    "mlp_model = None # Initialize\n",
    "\n",
    "# Check if Checkpoint 5 exists\n",
    "if USE_MLP and os.path.exists(CP5_MLP_MODEL_PATH):\n",
    "    print(\"Loading MLP model from Checkpoint 5...\")\n",
    "    try:\n",
    "        with open(CP5_MLP_MODEL_PATH, 'rb') as f:\n",
    "            mlp_model = cloudpickle.load(f)\n",
    "        fitted_transformers['mlp_model'] = mlp_model\n",
    "        print(\"MLP model loaded.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading MLP model from checkpoint: {e}. Will retrain if USE_MLP is True.\")\n",
    "        mlp_model = None\n",
    "\n",
    "# Train MLP if configured and not loaded from checkpoint\n",
    "if USE_MLP and mlp_model is None:\n",
    "    try:\n",
    "        mlp_model = train_mlp(train, feature_cols, MAIN_TARGET, ERA_COL, original_feature_cols)\n",
    "        # Save Checkpoint 5\n",
    "        print(\"Saving Checkpoint 5...\")\n",
    "        with open(CP5_MLP_MODEL_PATH, 'wb') as f:\n",
    "            cloudpickle.dump(mlp_model, f)\n",
    "        fitted_transformers['mlp_model'] = mlp_model\n",
    "        print(\"Checkpoint 5 saved.\")\n",
    "    except ImportError:\n",
    "        print(\"PyTorch not found. Skipping MLP training. Set USE_MLP=False or install PyTorch.\")\n",
    "        USE_MLP = False\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during MLP training: {e}\")\n",
    "        mlp_model = None\n",
    "        USE_MLP = False\n",
    "elif not USE_MLP:\n",
    "    print(\"Skipping MLP Training based on configuration (USE_MLP=False).\")\n",
    "\n",
    "# --- MLP Evaluation (if trained/loaded) ---\n",
    "if USE_MLP and mlp_model is not None:\n",
    "    print(\"\\nGenerating MLP predictions on validation set for evaluation...\")\n",
    "    mlp_model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_features_tensor = torch.tensor(validation[feature_cols].fillna(0.5).values, dtype=torch.float32)\n",
    "        mlp_preds = mlp_model(val_features_tensor).numpy().squeeze()\n",
    "    validation[\"prediction_mlp\"] = mlp_preds\n",
    "    print(\"MLP predictions generated.\")\n",
    "    display(validation[[\"prediction_mlp\"]].head())\n",
    "    \n",
    "    print(\"\\nEvaluating MLP performance...\")\n",
    "    evaluation_cols_mlp = [f\"prediction_{MAIN_TARGET}\", \"prediction_mlp\"]\n",
    "    validation_eval_mlp = validation.dropna(subset=[MAIN_TARGET] + evaluation_cols_mlp)\n",
    "    \n",
    "    if validation_eval_mlp.empty:\n",
    "        print(\"Warning: No valid rows for MLP evaluation.\")\n",
    "    else:\n",
    "        mlp_correlations = validation_eval_mlp.groupby(ERA_COL).apply(\n",
    "            lambda d: numerai_corr(d[evaluation_cols_mlp], d[MAIN_TARGET])\n",
    "        )\n",
    "        mlp_cumsum_corrs = mlp_correlations.cumsum()\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        mlp_cumsum_corrs.plot(ax=plt.gca())\n",
    "        plt.title(\"Cumulative Correlation of MLP vs Base Model\")\n",
    "        plt.xlabel(\"Era\")\n",
    "        plt.ylabel(\"Cumulative Correlation\")\n",
    "        plt.xticks([])\n",
    "        plt.legend(title=\"Model\")\n",
    "        plt.grid(True, linestyle='--', alpha=0.5)\n",
    "        plt.show()\n",
    "\n",
    "        print(\"\\nSummary metrics for MLP:\")\n",
    "        mlp_summary = {}\n",
    "        for pred_col in evaluation_cols_mlp:\n",
    "             if pred_col in mlp_correlations.columns:\n",
    "               mlp_summary[pred_col] = get_summary_metrics(mlp_correlations[pred_col], mlp_cumsum_corrs[pred_col])\n",
    "             else:\n",
    "                mlp_summary[pred_col] = {'mean': np.nan, 'std': np.nan, 'sharpe': np.nan, 'max_drawdown': np.nan}\n",
    "        mlp_summary_df = pd.DataFrame(mlp_summary).T\n",
    "        display(mlp_summary_df)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "part6_md"
   },
   "source": [
    "## Part 6: Final Model Evaluation & Prediction Function\n",
    "\n",
    "Determine the final model based on configuration flags and evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "part6_code"
   },
   "outputs": [],
   "source": [
    "# Determine the final prediction column based on flags and successful execution\n",
    "if USE_MLP and mlp_model and \"prediction_mlp\" in validation.columns:\n",
    "    final_pred_col = \"prediction_mlp\"\n",
    "    comparison_cols = [f\"prediction_{MAIN_TARGET}\", final_pred_col]\n",
    "    print(f\"Final model selected: MLP ({final_pred_col})\")\n",
    "elif USE_STACKING and meta_model and \"prediction_stacked\" in validation.columns:\n",
    "    final_pred_col = \"prediction_stacked\"\n",
    "    comparison_cols = prediction_cols + [final_pred_col] # Compare stacker to base models\n",
    "    print(f\"Final model selected: Stacked Ensemble ({final_pred_col})\")\n",
    "else:\n",
    "    final_pred_col = f\"prediction_{MAIN_TARGET}\"\n",
    "    comparison_cols = [final_pred_col]\n",
    "    print(f\"Final model selected: Base model for {MAIN_TARGET} ({final_pred_col})\")\n",
    "    if final_pred_col not in validation.columns:\n",
    "         raise ValueError(\"Could not find a valid prediction column for final evaluation.\")\n",
    "\n",
    "# Ensure all columns for comparison exist\n",
    "existing_comparison_cols = [col for col in comparison_cols if col in validation.columns]\n",
    "if not existing_comparison_cols:\n",
    "     raise ValueError(\"No valid columns found for final comparison evaluation.\")\n",
    "\n",
    "print(f\"\\nEvaluating final model performance (comparing: {existing_comparison_cols})...\")\n",
    "validation_eval_final = validation.dropna(subset=[MAIN_TARGET] + existing_comparison_cols)\n",
    "\n",
    "if validation_eval_final.empty:\n",
    "    print(\"Warning: No valid rows for final evaluation.\")\n",
    "else:\n",
    "    final_correlations = validation_eval_final.groupby(ERA_COL).apply(\n",
    "        lambda d: numerai_corr(d[existing_comparison_cols], d[MAIN_TARGET])\n",
    "    )\n",
    "    final_cumsum_corrs = final_correlations.cumsum()\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    final_cumsum_corrs.plot(ax=plt.gca())\n",
    "    plt.title(f\"Cumulative Correlation of Final Model ({final_pred_col}) vs Others\")\n",
    "    plt.xlabel(\"Era\")\n",
    "    plt.ylabel(\"Cumulative Correlation\")\n",
    "    plt.xticks([])\n",
    "    plt.legend(title=\"Model\")\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nFinal Summary Metrics:\")\n",
    "    final_summary = {}\n",
    "    for pred_col in existing_comparison_cols:\n",
    "        if pred_col in final_correlations.columns:\n",
    "            final_summary[pred_col] = get_summary_metrics(final_correlations[pred_col], final_cumsum_corrs[pred_col])\n",
    "        else:\n",
    "             final_summary[pred_col] = {'mean': np.nan, 'std': np.nan, 'sharpe': np.nan, 'max_drawdown': np.nan}\n",
    "    final_summary_df = pd.DataFrame(final_summary).T\n",
    "    display(final_summary_df)\n",
    "\n",
    "# --- Define Final Prediction Function ---\n",
    "# This function now relies on the 'dependencies' dict passed during pickling/loading\n",
    "def predict_final(live_features: pd.DataFrame, dependencies: dict) -> pd.DataFrame:\n",
    "    \"\"\"Generates predictions using the chosen final model and pre-fitted FE objects.\"\"\"\n",
    "    print(\"Starting final prediction function...\")\n",
    "    # Load dependencies\n",
    "    original_feature_cols = dependencies['original_feature_cols']\n",
    "    feature_cols = dependencies['feature_cols']\n",
    "    umap_reducer = dependencies.get('umap_reducer')\n",
    "    ae_encoder = dependencies.get('ae_encoder')\n",
    "    qt = dependencies.get('qt')\n",
    "    synthetic_mean = dependencies.get('synthetic_mean')\n",
    "    # Reconstruct feature names from dependencies if needed\n",
    "    umap_feature_names = [f\"umap_feat_{i}\" for i in range(dependencies['UMAP_N_COMPONENTS'])] if umap_reducer else []\n",
    "    ae_feature_names = [f\"ae_feat_{i}\" for i in range(dependencies['AE_ENCODING_DIM'])] if ae_encoder else []\n",
    "    contrastive_feature_names = [f\"contrastive_feat_{i}\" for i in range(dependencies['CONTRASTIVE_EMB_DIM'])]\n",
    "    ctgan_feature_names = [f\"dist_to_synth_mean_{dependencies['MAIN_TARGET']}\"] if dependencies.get('ctgan_model') else []\n",
    "\n",
    "    models_dep = dependencies['models']\n",
    "    meta_model_dep = dependencies.get('meta_model')\n",
    "    scaler_dep = dependencies.get('stacking_scaler')\n",
    "    mlp_model_dep = dependencies.get('mlp_model')\n",
    "    USE_MLP_FLAG = dependencies['USE_MLP']\n",
    "    USE_STACKING_FLAG = dependencies['USE_STACKING']\n",
    "    STACKING_MODEL_TYPE_FLAG = dependencies['STACKING_MODEL_TYPE']\n",
    "    MAIN_TARGET_NAME = dependencies['MAIN_TARGET']\n",
    "    PREDICTION_COL_NAME = dependencies['PREDICTION_COL']\n",
    "\n",
    "    # Apply feature engineering transformations\n",
    "    print(\"Applying feature engineering to live data...\")\n",
    "    live_features_eng = live_features.copy()\n",
    "    live_data_orig = live_features_eng[original_feature_cols].astype(np.float32).fillna(0.5)\n",
    "\n",
    "    if umap_reducer and umap_feature_names:\n",
    "        print(\" Applying UMAP...\")\n",
    "        umap_features_live = umap_reducer.transform(live_data_orig).astype(np.float32)\n",
    "        live_features_eng[umap_feature_names] = umap_features_live\n",
    "    if ae_encoder and ae_feature_names:\n",
    "        print(\" Applying AE...\")\n",
    "        ae_features_live = ae_encoder.predict(live_data_orig.values).astype(np.float32)\n",
    "        live_features_eng[ae_feature_names] = ae_features_live\n",
    "    if contrastive_feature_names:\n",
    "        print(\" Applying Contrastive (placeholder)...\")\n",
    "        num_samples_live = len(live_features_eng)\n",
    "        contrastive_features_live = np.random.rand(num_samples_live, len(contrastive_feature_names)).astype(np.float32)\n",
    "        live_features_eng[contrastive_feature_names] = contrastive_features_live\n",
    "    if ctgan_feature_names and synthetic_mean is not None:\n",
    "        print(\" Applying CTGAN distance feature...\")\n",
    "        distances_live = np.linalg.norm(live_data_orig.values - synthetic_mean.values, axis=1).astype(np.float32)\n",
    "        live_features_eng[ctgan_feature_names[0]] = distances_live\n",
    "    print(\"Feature engineering applied.\")\n",
    "\n",
    "    # Ensure all feature columns exist\n",
    "    for col in feature_cols:\n",
    "        if col not in live_features_eng.columns:\n",
    "            live_features_eng[col] = 0.5 # Fill missing engineered features\n",
    "    # Convert dtypes for consistency before prediction\n",
    "    for col in original_feature_cols:\n",
    "        if col in live_features_eng.columns:\n",
    "             live_features_eng[col] = live_features_eng[col].astype(np.int8)\n",
    "    for col in engineered_feature_cols:\n",
    "         if col in live_features_eng.columns:\n",
    "             live_features_eng[col] = live_features_eng[col].astype(np.float32)\n",
    "             \n",
    "    live_features_eng = live_features_eng[feature_cols].fillna(0.5)\n",
    "\n",
    "    # Prediction Logic\n",
    "    if USE_MLP_FLAG and mlp_model_dep:\n",
    "        print(\"Generating predictions using MLP model...\")\n",
    "        mlp_model_dep.eval()\n",
    "        with torch.no_grad():\n",
    "            live_features_tensor = torch.tensor(live_features_eng.values, dtype=torch.float32)\n",
    "            predictions = mlp_model_dep(live_features_tensor).numpy().squeeze()\n",
    "        submission_df = pd.DataFrame({'prediction': predictions}, index=live_features.index)\n",
    "    elif USE_STACKING_FLAG and meta_model_dep:\n",
    "        print(\"Generating predictions using Stacked Ensemble...\")\n",
    "        base_preds_live = pd.DataFrame(index=live_features.index)\n",
    "        oof_cols = [f\"oof_{t}\" for t in models_dep.keys()]\n",
    "        for target_name, model in models_dep.items():\n",
    "            base_preds_live[f\"oof_{target_name}\"] = model.predict(live_features_eng)\n",
    "        base_preds_live = base_preds_live.fillna(base_preds_live.mean())\n",
    "        if STACKING_MODEL_TYPE_FLAG == 'Linear' and scaler_dep:\n",
    "             base_preds_live_scaled = scaler_dep.transform(base_preds_live[oof_cols])\n",
    "             stacked_preds_live = meta_model_dep.predict(base_preds_live_scaled)\n",
    "        else:\n",
    "             stacked_preds_live = meta_model_dep.predict(base_preds_live[oof_cols])\n",
    "        submission_df = pd.DataFrame({'prediction': stacked_preds_live}, index=live_features.index)\n",
    "    else:\n",
    "        print(f\"Generating predictions using base model for {MAIN_TARGET_NAME}...\")\n",
    "        predictions = models_dep[MAIN_TARGET_NAME].predict(live_features_eng)\n",
    "        submission_df = pd.DataFrame({'prediction': predictions}, index=live_features.index)\n",
    "\n",
    "    ranked_submission = submission_df['prediction'].rank(pct=True, method=\"first\")\n",
    "    print(\"Final predictions generated and ranked.\")\n",
    "    return ranked_submission.to_frame(PREDICTION_COL_NAME)\n",
    "\n",
    "print(\"Final prediction function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "part7_md"
   },
   "source": [
    "## Part 7: Model Pickling for Upload\n",
    "\n",
    "Pickle the final prediction function and its dependencies. \n",
    "**Important:** When running this notebook in parts, ensure all necessary objects (from previous checkpoints) are loaded into the current session's `fitted_transformers` and `models` dictionaries before executing this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "part7_code"
   },
   "outputs": [],
   "source": [
    "# --- Quick Test on Live Data (using the final function) ---\n",
    "print(\"Downloading live features for testing...\")\n",
    "napi.download_dataset(f\"{DATA_VERSION}/live.parquet\")\n",
    "live_features = pd.read_parquet(f\"{DATA_VERSION}/live.parquet\", columns=original_feature_cols).set_index(ID_COL)\n",
    "\n",
    "# Prepare dependencies dictionary for the test\n",
    "# Ensure all required fitted objects are loaded into fitted_transformers from checkpoints\n",
    "test_dependencies = {\n",
    "    'models': models,\n",
    "    'umap_reducer': fitted_transformers.get('umap_reducer'),\n",
    "    'ae_encoder': fitted_transformers.get('ae_encoder'),\n",
    "    'qt': fitted_transformers.get('qt'),\n",
    "    'synthetic_mean': fitted_transformers.get('synthetic_mean'),\n",
    "    'meta_model': fitted_transformers.get('meta_model'),\n",
    "    'stacking_scaler': fitted_transformers.get('stacking_scaler'),\n",
    "    'mlp_model': fitted_transformers.get('mlp_model'),\n",
    "    'original_feature_cols': original_feature_cols,\n",
    "    'engineered_feature_cols': engineered_feature_cols,\n",
    "    'feature_cols': feature_cols,\n",
    "    'ae_feats': [f\"ae_feat_{i}\" for i in range(AE_ENCODING_DIM)] if fitted_transformers.get('ae_encoder') else [], \n",
    "    'umap_feats': [f\"umap_feat_{i}\" for i in range(UMAP_N_COMPONENTS)] if fitted_transformers.get('umap_reducer') else [],\n",
    "    'contrastive_feats': [f\"contrastive_feat_{i}\" for i in range(CONTRASTIVE_EMB_DIM)],\n",
    "    'ctgan_feats': [f\"dist_to_synth_mean_{MAIN_TARGET}\"] if fitted_transformers.get('ctgan_model') else [],\n",
    "    'UMAP_N_COMPONENTS': UMAP_N_COMPONENTS,\n",
    "    'AE_ENCODING_DIM': AE_ENCODING_DIM,\n",
    "    'CONTRASTIVE_EMB_DIM': CONTRASTIVE_EMB_DIM,\n",
    "    'USE_MLP': USE_MLP,\n",
    "    'USE_STACKING': USE_STACKING,\n",
    "    'STACKING_MODEL_TYPE': STACKING_MODEL_TYPE,\n",
    "    'MAIN_TARGET': MAIN_TARGET,\n",
    "    'PREDICTION_COL': PREDICTION_COL\n",
    "}\n",
    "\n",
    "# Generate predictions using the final function\n",
    "final_predictions = predict_final(live_features, test_dependencies)\n",
    "\n",
    "print(\"\\nSample of final predictions:\")\n",
    "display(final_predictions.head())\n",
    "\n",
    "# --- Pickle the Prediction Function and Dependencies ---\n",
    "print(\"\\nPickling the prediction function...\")\n",
    "try:\n",
    "    # Define the dictionary containing the function and its necessary dependencies\n",
    "    # Use the same 'test_dependencies' dictionary structure\n",
    "    pickle_payload = {\n",
    "        'predict_fn': predict_final,\n",
    "        'dependencies': test_dependencies # Pass the constructed dictionary\n",
    "    }\n",
    "    \n",
    "    # Register libraries that cloudpickle might struggle with by default\n",
    "    cloudpickle.register_pickle_by_value(umap)\n",
    "    cloudpickle.register_pickle_by_value(tf)\n",
    "    cloudpickle.register_pickle_by_value(torch)\n",
    "    cloudpickle.register_pickle_by_value(ctgan)\n",
    "    \n",
    "    # Pickle the payload\n",
    "    with open(\"predict_final_model.pkl\", \"wb\") as f:\n",
    "        cloudpickle.dump(pickle_payload, f)\n",
    "    print(\"Prediction function and dependencies pickled successfully to predict_final_model.pkl\")\n",
    "\n",
    "except NameError as e:\n",
    "    print(f\"Pickling failed: A required object might not be defined. Error: {e}\")\n",
    "    print(\"Ensure all models and transformers used in 'predict_final' are trained/loaded and available in 'test_dependencies'.\")\n",
    "except Exception as e:\n",
    "     print(f\"An unexpected error occurred during pickling: {e}\")\n",
    "\n",
    "# --- Download Final Pickle File ---\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download('predict_final_model.pkl')\n",
    "except ImportError:\n",
    "    print(\"\\nSkipping download (not in Colab environment).\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nFile download failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "part8_md"
   },
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "This notebook demonstrated adding feature engineering, stacked ensembling, and an optional era-invariant MLP training pipeline, structured with checkpoints.\n",
    "\n",
    "To use the checkpoints:\n",
    "* Run the cells within each 'Part' sequentially.\n",
    "* If you stop and restart the kernel/environment, re-run the 'Setup & Configuration' cell first.\n",
    "* Then, run the cell for the 'Part' you want to start from. It will attempt to load from the previous part's checkpoint.\n",
    "* **Crucially**, if you load from checkpoints, ensure you run all subsequent parts up to Part 7 before pickling, so all necessary models and transformers are loaded into memory for the `predict_final` function.\n",
    "\n",
    "Remember to choose the model (Stacking or MLP) you want to submit by setting the `USE_STACKING` or `USE_MLP` flags before pickling and uploading `predict_final_model.pkl` to [numer.ai](https://numer.ai)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
