{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FOUvgVp3xnNW"
   },
   "source": [
    "# Numerai Modeling: Feature Engineering, Ensembling, and Advanced Training\n",
    "\n",
    "This notebook demonstrates several advanced modeling techniques for the Numerai tournament:\n",
    "1. **Feature Engineering**: Creating new features using UMAP, Denoising Autoencoders, Contrastive Learning, and CTGAN.\n",
    "2. **Target Exploration**: Analyzing auxiliary targets.\n",
    "3. **Base Model Training**: Training LightGBM models on different targets and engineered features.\n",
    "4. **Stacked Ensembling**: Combining base model predictions using a meta-model.\n",
    "5. **Era-Invariant Training**: Using a PyTorch MLP with custom loss functions (correlation, era variance penalty, feature exposure penalty).\n",
    "6. **Model Selection & Upload**: Choosing the final model and preparing for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KD826S8uxnNY",
    "outputId": "418275f7-0333-4e08-db73-2407cc9afe3e"
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip3 install -q numerapi pandas pyarrow matplotlib lightgbm scikit-learn cloudpickle==2.2.1 seaborn scipy==1.10.1 umap-learn tensorflow torch ctgan tqdm\n",
    "\n",
    "# Inline plots\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config_cell"
   },
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "config_vars"
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "dlopen(/Users/I570611/Documents/GitHub/BarrierOptions/.conda/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.dylib, 0x0006): Library not loaded: @rpath/libomp.dylib\n  Referenced from: <8FC36893-94B8-343C-9D9F-4CCBFE81B89B> /Users/I570611/Documents/GitHub/BarrierOptions/.conda/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.dylib\n  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/opt/local/lib/libomp/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/local/lib/libomp/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/opt/local/lib/libomp/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/local/lib/libomp/libomp.dylib' (no such file), '/Users/I570611/Documents/GitHub/BarrierOptions/.conda/lib/python3.11/lib-dynload/../../libomp.dylib' (no such file), '/Users/I570611/Documents/GitHub/BarrierOptions/.conda/bin/../lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgc\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumerapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NumerAPI\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlightgbm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlgb\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GroupKFold\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinear_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Ridge\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/BarrierOptions/.conda/lib/python3.11/site-packages/lightgbm/__init__.py:11\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# .basic is intentionally loaded as early as possible, to dlopen() lib_lightgbm.{dll,dylib,so}\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# and its dependencies as early as possible\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbasic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Booster, Dataset, Sequence, register_logger\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcallback\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EarlyStopException, early_stopping, log_evaluation, record_evaluation, reset_parameter\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mengine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CVBooster, cv, train\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/BarrierOptions/.conda/lib/python3.11/site-packages/lightgbm/basic.py:9\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[33;03m\"\"\"Wrapper for C API of LightGBM.\"\"\"\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# This import causes lib_lightgbm.{dll,dylib,so} to be loaded.\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# It's intentionally done here, as early as possible, to avoid issues like\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# \"libgomp.so.1: cannot allocate memory in static TLS block\" on aarch64 Linux.\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# For details, see the \"cannot allocate memory in static TLS block\" entry in docs/FAQ.rst.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlibpath\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _LIB  \u001b[38;5;66;03m# isort: skip\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mabc\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mctypes\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/BarrierOptions/.conda/lib/python3.11/site-packages/lightgbm/libpath.py:49\u001b[39m\n\u001b[32m     47\u001b[39m     _LIB = Mock(ctypes.CDLL)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     _LIB = \u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcdll\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLoadLibrary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_find_lib_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/BarrierOptions/.conda/lib/python3.11/ctypes/__init__.py:454\u001b[39m, in \u001b[36mLibraryLoader.LoadLibrary\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mLoadLibrary\u001b[39m(\u001b[38;5;28mself\u001b[39m, name):\n\u001b[32m--> \u001b[39m\u001b[32m454\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dlltype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/BarrierOptions/.conda/lib/python3.11/ctypes/__init__.py:376\u001b[39m, in \u001b[36mCDLL.__init__\u001b[39m\u001b[34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[39m\n\u001b[32m    373\u001b[39m \u001b[38;5;28mself\u001b[39m._FuncPtr = _FuncPtr\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m376\u001b[39m     \u001b[38;5;28mself\u001b[39m._handle = \u001b[43m_dlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    378\u001b[39m     \u001b[38;5;28mself\u001b[39m._handle = handle\n",
      "\u001b[31mOSError\u001b[39m: dlopen(/Users/I570611/Documents/GitHub/BarrierOptions/.conda/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.dylib, 0x0006): Library not loaded: @rpath/libomp.dylib\n  Referenced from: <8FC36893-94B8-343C-9D9F-4CCBFE81B89B> /Users/I570611/Documents/GitHub/BarrierOptions/.conda/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.dylib\n  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/opt/local/lib/libomp/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/local/lib/libomp/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/opt/local/lib/libomp/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/local/lib/libomp/libomp.dylib' (no such file), '/Users/I570611/Documents/GitHub/BarrierOptions/.conda/lib/python3.11/lib-dynload/../../libomp.dylib' (no such file), '/Users/I570611/Documents/GitHub/BarrierOptions/.conda/bin/../lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import gc\n",
    "from numerapi import NumerAPI\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import cloudpickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# --- Configuration ---\n",
    "DATA_VERSION = \"v5.0\"\n",
    "MAIN_TARGET = \"target_cyrusd_20\"\n",
    "AUX_TARGETS = [\n",
    "  \"target_victor_20\",\n",
    "  \"target_xerxes_20\",\n",
    "  \"target_teager2b_20\"\n",
    "]\n",
    "TARGET_CANDIDATES = [MAIN_TARGET] + AUX_TARGETS\n",
    "ERA_COL = \"era\"\n",
    "DATA_TYPE_COL = \"data_type\"\n",
    "TARGET_COL = \"target\" # Alias for MAIN_TARGET in original notebook\n",
    "PREDICTION_COL = \"prediction\"\n",
    "\n",
    "# Feature Engineering Hyperparameters\n",
    "UMAP_N_COMPONENTS = 50\n",
    "AE_ENCODING_DIM = 64\n",
    "CONTRASTIVE_EMB_DIM = 64\n",
    "CTGAN_EPOCHS = 100 # Reduced for speed in example\n",
    "\n",
    "# Stacking Ensemble Config\n",
    "N_FOLDS = 5 # Number of folds for OOF predictions\n",
    "STACKING_MODEL_TYPE = 'LGBM' # 'LGBM' or 'Linear'\n",
    "\n",
    "# PyTorch MLP Config\n",
    "MLP_EPOCHS = 10 # Reduced for speed in example\n",
    "MLP_BATCH_SIZE = 512\n",
    "MLP_LR = 0.001\n",
    "VARIANCE_PENALTY_WEIGHT = 0.01 # lambda1\n",
    "FEATURE_EXPOSURE_WEIGHT = 0.01 # lambda2\n",
    "TOP_N_FEATURES_FOR_EXPOSURE = 50 # Use top N features for exposure penalty\n",
    "\n",
    "# Model Selection Flags\n",
    "USE_STACKING = True # Set to True to use Stacking, False for MLP\n",
    "USE_MLP = False\n",
    "\n",
    "# Downsampling for faster execution (optional)\n",
    "DOWNSAMPLE_TRAIN_ERAS = 4 # Use every Nth era for training\n",
    "DOWNSAMPLE_VALID_ERAS = 4 # Use every Nth era for validation\n",
    "\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', lambda x: f'{x:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "feature_eng_funcs_md"
   },
   "source": [
    "## 1. Feature Engineering Functions\n",
    "\n",
    "Define functions to generate new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "feature_eng_funcs_code"
   },
   "outputs": [],
   "source": [
    "import umap\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from ctgan import CTGAN\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "def umap_feature_creation(df, feature_cols, n_components=UMAP_N_COMPONENTS, random_state=42):\n",
    "    \"\"\"Creates features using UMAP.\"\"\"\n",
    "    print(f\"Creating {n_components} UMAP features...\")\n",
    "    reducer = umap.UMAP(n_components=n_components, random_state=random_state)\n",
    "    # Ensure data is float32 and handle potential NaNs (though features shouldn't have NaNs)\n",
    "    umap_features = reducer.fit_transform(df[feature_cols].astype(np.float32).fillna(0.5))\n",
    "    umap_feature_names = [f\"umap_feat_{i}\" for i in range(n_components)]\n",
    "    df[umap_feature_names] = umap_features\n",
    "    print(\"UMAP features created.\")\n",
    "    return df, umap_feature_names\n",
    "\n",
    "def denoising_autoencoder_features(df, feature_cols, encoding_dim=AE_ENCODING_DIM, noise_factor=0.1, epochs=10, batch_size=256):\n",
    "    \"\"\"Creates features using a Denoising Autoencoder (Keras).\"\"\"\n",
    "    print(f\"Creating {encoding_dim} Denoising AE features...\")\n",
    "    input_dim = len(feature_cols)\n",
    "    data = df[feature_cols].astype(np.float32).fillna(0.5).values\n",
    "\n",
    "    # Add noise\n",
    "    noisy_data = data + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=data.shape)\n",
    "    noisy_data = np.clip(noisy_data, 0., 1.) # Assuming features are scaled 0-1\n",
    "\n",
    "    # Define Autoencoder\n",
    "    input_layer = keras.Input(shape=(input_dim,))\n",
    "    encoded = layers.Dense(encoding_dim * 2, activation='relu')(input_layer)\n",
    "    encoded = layers.Dense(encoding_dim, activation='relu')(encoded)\n",
    "    decoded = layers.Dense(encoding_dim * 2, activation='relu')(encoded)\n",
    "    decoded = layers.Dense(input_dim, activation='sigmoid')(decoded) # Sigmoid for [0, 1] output\n",
    "\n",
    "    autoencoder = keras.Model(input_layer, decoded)\n",
    "    encoder = keras.Model(input_layer, encoded)\n",
    "\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    # Train Autoencoder\n",
    "    autoencoder.fit(noisy_data, data, # Train to reconstruct original from noisy\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    validation_split=0.1, # Use a small validation split\n",
    "                    verbose=0) # Suppress verbose output for brevity\n",
    "\n",
    "    # Get encoded features\n",
    "    ae_features = encoder.predict(data)\n",
    "    ae_feature_names = [f\"ae_feat_{i}\" for i in range(encoding_dim)]\n",
    "    df[ae_feature_names] = ae_features\n",
    "    print(\"AE features created.\")\n",
    "    return df, ae_feature_names\n",
    "\n",
    "def contrastive_feature_creation(df, feature_cols, embedding_dim=CONTRASTIVE_EMB_DIM, epochs=5, batch_size=256):\n",
    "    \"\"\"Placeholder for contrastive learning feature creation (generates random embeddings).\"\"\"\n",
    "    # NOTE: A proper implementation requires defining positive/negative pairs (e.g., based on eras or targets)\n",
    "    # and training a model (like a Siamese network) with a contrastive loss (e.g., TripletLoss).\n",
    "    # This placeholder generates random features for demonstration.\n",
    "    print(f\"Creating {embedding_dim} Contrastive (placeholder) features...\")\n",
    "    num_samples = len(df)\n",
    "    contrastive_features = np.random.rand(num_samples, embedding_dim).astype(np.float32)\n",
    "    contrastive_feature_names = [f\"contrastive_feat_{i}\" for i in range(embedding_dim)]\n",
    "    df[contrastive_feature_names] = contrastive_features\n",
    "    print(\"Contrastive (placeholder) features created.\")\n",
    "    return df, contrastive_feature_names\n",
    "\n",
    "def synthetic_data_ctgan(df, feature_cols, target_col, n_synthetic_samples_ratio=0.5, epochs=CTGAN_EPOCHS):\n",
    "    \"\"\"Generates synthetic features using CTGAN based on original features and a target.\"\"\"\n",
    "    print(f\"Creating synthetic features using CTGAN (target: {target_col})...\")\n",
    "    \n",
    "    # CTGAN works best with discrete or transformed continuous data.\n",
    "    # We'll use QuantileTransformer as features are somewhat ordinal.\n",
    "    data_subset = df[feature_cols + [target_col]].copy().dropna(subset=[target_col])\n",
    "    \n",
    "    # Handle potential NaNs in features (though ideally preprocessed)\n",
    "    data_subset[feature_cols] = data_subset[feature_cols].fillna(0.5)\n",
    "\n",
    "    # Transform features to be more Gaussian-like (helps CTGAN)\n",
    "    qt = QuantileTransformer(output_distribution='normal', random_state=42)\n",
    "    data_transformed = qt.fit_transform(data_subset[feature_cols])\n",
    "    data_transformed_df = pd.DataFrame(data_transformed, columns=feature_cols, index=data_subset.index)\n",
    "    data_transformed_df[target_col] = data_subset[target_col].values # Add target back\n",
    "\n",
    "    # Define discrete columns (none in this case, but important for CTGAN)\n",
    "    discrete_columns = []\n",
    "\n",
    "    # Train CTGAN\n",
    "    ctgan = CTGAN(verbose=False)\n",
    "    try:\n",
    "        ctgan.fit(data_transformed_df, discrete_columns, epochs=epochs)\n",
    "    except Exception as e:\n",
    "        print(f\"CTGAN fitting failed: {e}. Skipping CTGAN features.\")\n",
    "        return df, []\n",
    "\n",
    "    # Generate synthetic samples\n",
    "    n_synthetic_samples = int(len(data_subset) * n_synthetic_samples_ratio)\n",
    "    synthetic_data_transformed = ctgan.sample(n_synthetic_samples)\n",
    "\n",
    "    # Inverse transform the synthetic features\n",
    "    synthetic_features_original_scale = qt.inverse_transform(synthetic_data_transformed[feature_cols])\n",
    "    synthetic_df = pd.DataFrame(synthetic_features_original_scale, columns=feature_cols)\n",
    "    \n",
    "    # For simplicity, we'll create features based on the distance to synthetic samples.\n",
    "    # A more advanced approach might use synthetic data for augmentation or training separate models.\n",
    "    # Calculate mean of synthetic features as a simple derived feature\n",
    "    synthetic_mean = synthetic_df.mean(axis=0)\n",
    "    ctgan_feature_name = f\"dist_to_synth_mean_{target_col}\"\n",
    "    \n",
    "    # Calculate Euclidean distance for each original sample to the mean synthetic sample\n",
    "    original_features = df[feature_cols].fillna(0.5).values\n",
    "    distances = np.linalg.norm(original_features - synthetic_mean.values.astype(np.float32), axis=1)\n",
    "    \n",
    "    df[ctgan_feature_name] = distances\n",
    "    print(\"CTGAN-derived features created.\")\n",
    "    \n",
    "    del data_subset, data_transformed, data_transformed_df, synthetic_data_transformed, synthetic_features_original_scale, synthetic_df\n",
    "    gc.collect()\n",
    "    \n",
    "    return df, [ctgan_feature_name]\n",
    "\n",
    "print(\"Feature engineering functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VwChLrKexnNa"
   },
   "source": [
    "## 2. Data Loading and Initial Exploration\n",
    "\n",
    "Load training data and explore auxiliary targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 511
    },
    "id": "R1I_xkY4xnNa",
    "outputId": "e65c1cff-8db6-4a21-8081-b256026f6f3a"
   },
   "outputs": [],
   "source": [
    "napi = NumerAPI()\n",
    "\n",
    "# Download metadata and training data\n",
    "print(\"Downloading metadata...\")\n",
    "napi.download_dataset(f\"{DATA_VERSION}/features.json\")\n",
    "print(\"Downloading training data...\")\n",
    "napi.download_dataset(f\"{DATA_VERSION}/train.parquet\")\n",
    "\n",
    "# Load feature metadata and define feature sets\n",
    "print(\"Loading feature metadata...\")\n",
    "feature_metadata = json.load(open(f\"{DATA_VERSION}/features.json\"))\n",
    "feature_sets = feature_metadata[\"feature_sets\"]\n",
    "original_feature_cols = feature_sets[\"small\"] # Start with small features\n",
    "# original_feature_cols = feature_sets[\"medium\"] # Use medium for potentially better performance\n",
    "# original_feature_cols = feature_sets[\"all\"] # Use all for potentially better performance\n",
    "target_cols = feature_metadata[\"targets\"]\n",
    "\n",
    "# Load training data\n",
    "print(\"Loading training data...\")\n",
    "train = pd.read_parquet(\n",
    "    f\"{DATA_VERSION}/train.parquet\",\n",
    "    columns=[ERA_COL, DATA_TYPE_COL] + original_feature_cols + target_cols\n",
    ")\n",
    "\n",
    "# Filter for training data type (just in case)\n",
    "train = train[train[DATA_TYPE_COL] == \"train\"].copy()\n",
    "del train[DATA_TYPE_COL]\n",
    "gc.collect()\n",
    "\n",
    "# Downsample eras if configured\n",
    "if DOWNSAMPLE_TRAIN_ERAS > 1:\n",
    "    print(f\"Downsampling training data to every {DOWNSAMPLE_TRAIN_ERAS}th era...\")\n",
    "    train = train[train[ERA_COL].isin(train[ERA_COL].unique()[::DOWNSAMPLE_TRAIN_ERAS])]\n",
    "    gc.collect()\n",
    "\n",
    "# Display target columns\n",
    "print(\"Target columns in training data:\")\n",
    "display(train[[ERA_COL] + target_cols].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4YzbRO5uxnNa"
   },
   "source": [
    "### The Main Target\n",
    "\n",
    "The primary target for Numerai predictions is typically `target_cyrusd_20`. The `target` column is often an alias for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pP6LnWcExnNa"
   },
   "outputs": [],
   "source": [
    "# Check if 'target' is an alias for the main target and prepare targets DataFrame\n",
    "if TARGET_COL in train.columns:\n",
    "    assert train[TARGET_COL].equals(train[MAIN_TARGET]), f\"'{TARGET_COL}' column is not equal to '{MAIN_TARGET}'\"\n",
    "    print(f\"'{TARGET_COL}' column confirmed as alias for '{MAIN_TARGET}'.\")\n",
    "    targets_df = train[[ERA_COL] + target_cols].drop(columns=[TARGET_COL])\n",
    "    target_cols.remove(TARGET_COL)\n",
    "else:\n",
    "    targets_df = train[[ERA_COL] + target_cols]\n",
    "\n",
    "print(f\"Using '{MAIN_TARGET}' as the main target.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d46TQDtrxnNb"
   },
   "source": [
    "### Target Names and Correlations\n",
    "\n",
    "Auxiliary targets represent different stock market return definitions or time horizons (`_20` vs `_60` days). They have varying correlations with the main target, which can be useful for ensembling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 645
    },
    "id": "P7uAdarxxnNb",
    "outputId": "e0972e03-0e96-4b35-f304-0531c4592530"
   },
   "outputs": [],
   "source": [
    "# Print target names grouped by name and time horizon\n",
    "t20s = sorted([t for t in target_cols if t.endswith(\"_20\")])\n",
    "t60s = sorted([t for t in target_cols if t.endswith(\"_60\")])\n",
    "names = sorted(list(set([t.replace(\"target_\", \"\").replace(\"_20\", \"\").replace(\"_60\", \"\") for t in target_cols])))\n",
    "\n",
    "target_map_df = pd.DataFrame(index=names, columns=['20', '60'])\n",
    "for t in t20s:\n",
    "    name = t.replace(\"target_\", \"\").replace(\"_20\", \"\")\n",
    "    target_map_df.loc[name, '20'] = t\n",
    "for t in t60s:\n",
    "    name = t.replace(\"target_\", \"\").replace(\"_60\", \"\")\n",
    "    target_map_df.loc[name, '60'] = t\n",
    "\n",
    "print(\"Target names grouped by name and horizon:\")\n",
    "display(target_map_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "target_corr_code"
   },
   "outputs": [],
   "source": [
    "# Calculate and display correlations with the main target\n",
    "print(f\"Correlations of auxiliary targets with {MAIN_TARGET}:\")\n",
    "target_corrs = (\n",
    "    targets_df[target_cols]\n",
    "    .corrwith(targets_df[MAIN_TARGET])\n",
    "    .sort_values(ascending=False)\n",
    "    .to_frame(f\"corr_with_{MAIN_TARGET}\")\n",
    ")\n",
    "display(target_corrs)\n",
    "\n",
    "# Plot correlation matrix heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "  targets_df[target_cols].corr(),\n",
    "  cmap=\"coolwarm\",\n",
    "  xticklabels=False,\n",
    "  yticklabels=False\n",
    ")\n",
    "plt.title(\"Target Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "apply_feature_eng_md"
   },
   "source": [
    "## 3. Apply Feature Engineering\n",
    "\n",
    "Generate UMAP, Denoising Autoencoder, Contrastive (placeholder), and CTGAN features for both training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "apply_feature_eng_code"
   },
   "outputs": [],
   "source": [
    "# --- Apply Feature Engineering to Training Data ---\n",
    "engineered_feature_cols = []\n",
    "\n",
    "# UMAP\n",
    "train, umap_feats = umap_feature_creation(train, original_feature_cols, n_components=UMAP_N_COMPONENTS)\n",
    "engineered_feature_cols.extend(umap_feats)\n",
    "gc.collect()\n",
    "\n",
    "# Denoising Autoencoder\n",
    "train, ae_feats = denoising_autoencoder_features(train, original_feature_cols, encoding_dim=AE_ENCODING_DIM)\n",
    "engineered_feature_cols.extend(ae_feats)\n",
    "gc.collect()\n",
    "\n",
    "# Contrastive Learning (Placeholder)\n",
    "train, contrastive_feats = contrastive_feature_creation(train, original_feature_cols, embedding_dim=CONTRASTIVE_EMB_DIM)\n",
    "engineered_feature_cols.extend(contrastive_feats)\n",
    "gc.collect()\n",
    "\n",
    "# CTGAN (using main target for demonstration)\n",
    "# Note: CTGAN can be computationally expensive\n",
    "train, ctgan_feats = synthetic_data_ctgan(train, original_feature_cols, MAIN_TARGET, epochs=CTGAN_EPOCHS)\n",
    "engineered_feature_cols.extend(ctgan_feats)\n",
    "gc.collect()\n",
    "\n",
    "# Update the main feature list\n",
    "feature_cols = original_feature_cols + engineered_feature_cols\n",
    "print(f\"\\nTotal number of features after engineering: {len(feature_cols)}\")\n",
    "print(f\"Engineered feature names: {engineered_feature_cols}\")\n",
    "\n",
    "# --- Load and Apply Feature Engineering to Validation Data ---\n",
    "print(\"\\nDownloading validation data...\")\n",
    "napi.download_dataset(f\"{DATA_VERSION}/validation.parquet\")\n",
    "\n",
    "print(\"Loading validation data...\")\n",
    "validation = pd.read_parquet(\n",
    "    f\"{DATA_VERSION}/validation.parquet\",\n",
    "    columns=[ERA_COL, DATA_TYPE_COL] + original_feature_cols + target_cols\n",
    ")\n",
    "validation = validation[validation[DATA_TYPE_COL] == \"validation\"].copy()\n",
    "del validation[DATA_TYPE_COL]\n",
    "gc.collect()\n",
    "\n",
    "# Downsample validation eras if configured\n",
    "if DOWNSAMPLE_VALID_ERAS > 1:\n",
    "    print(f\"Downsampling validation data to every {DOWNSAMPLE_VALID_ERAS}th era...\")\n",
    "    validation = validation[validation[ERA_COL].isin(validation[ERA_COL].unique()[::DOWNSAMPLE_VALID_ERAS])]\n",
    "    gc.collect()\n",
    "\n",
    "# Embargo overlapping eras\n",
    "last_train_era = int(train[ERA_COL].unique()[-1])\n",
    "eras_to_embargo = [str(era).zfill(4) for era in [last_train_era + i for i in range(1, 5)]] # Embargo 4 eras after last train era\n",
    "validation = validation[~validation[ERA_COL].isin(eras_to_embargo)].copy()\n",
    "print(f\"Embargoed eras: {eras_to_embargo}\")\n",
    "gc.collect()\n",
    "\n",
    "# Apply feature engineering transformations (using fitted models/transformers where applicable)\n",
    "# UMAP (Refit or use transform - refitting for simplicity here, but transform is better practice)\n",
    "validation, _ = umap_feature_creation(validation, original_feature_cols, n_components=UMAP_N_COMPONENTS)\n",
    "gc.collect()\n",
    "\n",
    "# Denoising Autoencoder (Use the trained encoder)\n",
    "print(\"Applying AE transformation to validation data...\")\n",
    "val_data_ae = validation[original_feature_cols].astype(np.float32).fillna(0.5).values\n",
    "ae_features_val = encoder.predict(val_data_ae)\n",
    "validation[ae_feats] = ae_features_val\n",
    "print(\"AE features applied to validation data.\")\n",
    "gc.collect()\n",
    "\n",
    "# Contrastive Learning (Placeholder - generate random features)\n",
    "validation, _ = contrastive_feature_creation(validation, original_feature_cols, embedding_dim=CONTRASTIVE_EMB_DIM)\n",
    "gc.collect()\n",
    "\n",
    "# CTGAN (Calculate distance feature for validation data)\n",
    "if ctgan_feats: # Only if CTGAN succeeded on train data\n",
    "    print(\"Applying CTGAN-derived feature calculation to validation data...\")\n",
    "    val_features_original = validation[original_feature_cols].fillna(0.5).values\n",
    "    distances_val = np.linalg.norm(val_features_original - synthetic_mean.values.astype(np.float32), axis=1)\n",
    "    validation[ctgan_feats[0]] = distances_val\n",
    "    print(\"CTGAN-derived features applied to validation data.\")\n",
    "else:\n",
    "    print(\"Skipping CTGAN features for validation data as it failed during training.\")\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\nValidation data with engineered features:\")\n",
    "display(validation[feature_cols].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "base_model_train_md"
   },
   "source": [
    "## 4. Base Model Training (LightGBM)\n",
    "\n",
    "Train LightGBM models for each selected target using the original and engineered features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VqBaYwmqxnNd",
    "outputId": "5da7bec6-e470-46a5-a553-b26e05a9084f"
   },
   "outputs": [],
   "source": [
    "print(\"Training LightGBM models on selected targets...\")\n",
    "models = {}\n",
    "for target in tqdm(TARGET_CANDIDATES, desc=\"Training models\"):\n",
    "    print(f\"Training model for {target}...\")\n",
    "    # Filter out rows where the current target is NaN for training\n",
    "    train_target_filtered = train.dropna(subset=[target])\n",
    "    \n",
    "    model = lgb.LGBMRegressor(\n",
    "        n_estimators=2000,\n",
    "        learning_rate=0.01,\n",
    "        max_depth=5,\n",
    "        num_leaves=2**4-1,\n",
    "        colsample_bytree=0.1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    model.fit(\n",
    "        train_target_filtered[feature_cols],\n",
    "        train_target_filtered[target]\n",
    "    )\n",
    "    models[target] = model\n",
    "    gc.collect()\n",
    "\n",
    "print(\"Base models trained.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "base_model_eval_md"
   },
   "source": [
    "### Base Model Evaluation\n",
    "\n",
    "Generate predictions on the validation set for each base model and evaluate their individual performance (Correlation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "base_model_eval_code"
   },
   "outputs": [],
   "source": [
    "from numerai_tools.scoring import numerai_corr\n",
    "\n",
    "print(\"Generating validation predictions for base models...\")\n",
    "validation_preds = pd.DataFrame(index=validation.index)\n",
    "for target_name, model in models.items():\n",
    "    pred_col_name = f\"prediction_{target_name}\"\n",
    "    validation_preds[pred_col_name] = model.predict(validation[feature_cols])\n",
    "\n",
    "# Merge predictions back into the validation dataframe\n",
    "validation = validation.join(validation_preds)\n",
    "\n",
    "prediction_cols = list(validation_preds.columns)\n",
    "print(\"\\nValidation predictions generated:\")\n",
    "display(validation[prediction_cols].head())\n",
    "\n",
    "# Evaluate individual model correlations\n",
    "print(\"\\nEvaluating base model correlations...\")\n",
    "correlations = validation.groupby(ERA_COL).apply(\n",
    "    lambda d: numerai_corr(d[prediction_cols], d[MAIN_TARGET])\n",
    ")\n",
    "cumsum_corrs = correlations.cumsum()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "cumsum_corrs.plot(ax=plt.gca())\n",
    "plt.title(\"Cumulative Correlation of Base Model Validation Predictions\")\n",
    "plt.xlabel(\"Era\")\n",
    "plt.ylabel(\"Cumulative Correlation\")\n",
    "plt.xticks([])\n",
    "plt.legend(title=\"Model Target\")\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSummary metrics for base models:\")\n",
    "def get_summary_metrics(scores, cumsum_scores):\n",
    "    summary_metrics = {}\n",
    "    mean = scores.mean()\n",
    "    std = scores.std()\n",
    "    sharpe = mean / std if std != 0 else np.nan\n",
    "    rolling_max = cumsum_scores.expanding(min_periods=1).max()\n",
    "    max_drawdown = (rolling_max - cumsum_scores).max()\n",
    "    return {\n",
    "        \"mean\": mean,\n",
    "        \"std\": std,\n",
    "        \"sharpe\": sharpe,\n",
    "        \"max_drawdown\": max_drawdown,\n",
    "    }\n",
    "\n",
    "base_model_summary = {}\n",
    "for pred_col in prediction_cols:\n",
    "    base_model_summary[pred_col] = get_summary_metrics(correlations[pred_col], cumsum_corrs[pred_col])\n",
    "\n",
    "summary_df = pd.DataFrame(base_model_summary).T\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "stacking_md"
   },
   "source": [
    "## 5. Stacked Ensembling\n",
    "\n",
    "Implement a stacked ensemble using out-of-fold predictions from the base models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "stacking_code"
   },
   "outputs": [],
   "source": [
    "print(\"Generating Out-of-Fold (OOF) predictions for stacking...\")\n",
    "\n",
    "gkf = GroupKFold(n_splits=N_FOLDS)\n",
    "oof_preds = pd.DataFrame(index=train.index)\n",
    "\n",
    "# Store OOF predictions for each base model\n",
    "for target_name, model in tqdm(models.items(), desc=\"Generating OOF preds\"):\n",
    "    print(f\" Generating OOF for {target_name}...\")\n",
    "    oof_preds_target = pd.Series(index=train.index, dtype=np.float32)\n",
    "    train_target_filtered = train.dropna(subset=[target_name]) # Use only non-NaN target rows for training folds\n",
    "    \n",
    "    for fold, (train_index, val_index) in enumerate(gkf.split(train_target_filtered[feature_cols], train_target_filtered[target_name], groups=train_target_filtered[ERA_COL])):\n",
    "        X_train_fold, X_val_fold = train_target_filtered.iloc[train_index][feature_cols], train_target_filtered.iloc[val_index][feature_cols]\n",
    "        y_train_fold, y_val_fold = train_target_filtered.iloc[train_index][target_name], train_target_filtered.iloc[val_index][target_name]\n",
    "        \n",
    "        fold_model = lgb.LGBMRegressor(\n",
    "            n_estimators=500, # Fewer estimators for fold training\n",
    "            learning_rate=0.01,\n",
    "            max_depth=5,\n",
    "            num_leaves=2**4-1,\n",
    "            colsample_bytree=0.1,\n",
    "            random_state=fold, # Vary random state per fold\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        fold_model.fit(X_train_fold, y_train_fold)\n",
    "        \n",
    "        # Store predictions on the validation part of the fold, using original train index\n",
    "        oof_preds_target.iloc[val_index] = fold_model.predict(X_val_fold)\n",
    "        \n",
    "    oof_preds[f\"oof_{target_name}\"] = oof_preds_target\n",
    "    gc.collect()\n",
    "\n",
    "print(\"OOF predictions generated.\")\n",
    "display(oof_preds.head())\n",
    "\n",
    "# Prepare training data for the meta-model\n",
    "meta_train_features = oof_preds.copy()\n",
    "meta_train_features[ERA_COL] = train[ERA_COL] # Add era for potential use in meta-model\n",
    "meta_train_target = train[MAIN_TARGET]\n",
    "\n",
    "# Drop rows where OOF preds or target are NaN (can happen if target was NaN in original data)\n",
    "valid_indices = meta_train_target.notna() & meta_train_features.notna().all(axis=1)\n",
    "meta_train_features = meta_train_features[valid_indices]\n",
    "meta_train_target = meta_train_target[valid_indices]\n",
    "\n",
    "oof_feature_cols = list(oof_preds.columns)\n",
    "\n",
    "# Train the meta-model\n",
    "print(f\"\\nTraining meta-model ({STACKING_MODEL_TYPE})...\")\n",
    "if STACKING_MODEL_TYPE == 'LGBM':\n",
    "    meta_model = lgb.LGBMRegressor(\n",
    "        n_estimators=500,\n",
    "        learning_rate=0.01,\n",
    "        max_depth=3,\n",
    "        num_leaves=2**3-1,\n",
    "        colsample_bytree=0.8, # Use more features for meta-model\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    meta_model.fit(meta_train_features[oof_feature_cols], meta_train_target)\n",
    "elif STACKING_MODEL_TYPE == 'Linear':\n",
    "    scaler = StandardScaler()\n",
    "    meta_train_features_scaled = scaler.fit_transform(meta_train_features[oof_feature_cols])\n",
    "    meta_model = Ridge(alpha=1.0, random_state=42)\n",
    "    meta_model.fit(meta_train_features_scaled, meta_train_target)\n",
    "else:\n",
    "    raise ValueError(\"Invalid STACKING_MODEL_TYPE\")\n",
    "\n",
    "print(\"Meta-model trained.\")\n",
    "\n",
    "# Generate predictions on the validation set using the stacking pipeline\n",
    "print(\"\\nGenerating stacked predictions on validation set...\")\n",
    "meta_val_features = validation[prediction_cols].copy()\n",
    "meta_val_features.columns = oof_feature_cols # Rename columns to match meta-model training\n",
    "\n",
    "if STACKING_MODEL_TYPE == 'Linear':\n",
    "    meta_val_features_scaled = scaler.transform(meta_val_features)\n",
    "    stacked_preds = meta_model.predict(meta_val_features_scaled)\n",
    "else: # LGBM\n",
    "    stacked_preds = meta_model.predict(meta_val_features)\n",
    "\n",
    "validation[\"prediction_stacked\"] = stacked_preds\n",
    "\n",
    "print(\"Stacked predictions generated.\")\n",
    "display(validation[[\"prediction_stacked\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "stacking_eval_md"
   },
   "source": [
    "### Stacked Ensemble Evaluation\n",
    "\n",
    "Evaluate the performance of the stacked ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "stacking_eval_code"
   },
   "outputs": [],
   "source": [
    "print(\"Evaluating stacked ensemble performance...\")\n",
    "\n",
    "# Add stacked predictions to the list for evaluation\n",
    "evaluation_cols = prediction_cols + [\"prediction_stacked\"]\n",
    "\n",
    "stacked_correlations = validation.groupby(ERA_COL).apply(\n",
    "    lambda d: numerai_corr(d[evaluation_cols], d[MAIN_TARGET])\n",
    ")\n",
    "stacked_cumsum_corrs = stacked_correlations.cumsum()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "stacked_cumsum_corrs.plot(ax=plt.gca())\n",
    "plt.title(\"Cumulative Correlation including Stacked Ensemble\")\n",
    "plt.xlabel(\"Era\")\n",
    "plt.ylabel(\"Cumulative Correlation\")\n",
    "plt.xticks([])\n",
    "plt.legend(title=\"Model\")\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSummary metrics including Stacked Ensemble:\")\n",
    "stacked_summary = {}\n",
    "for pred_col in evaluation_cols:\n",
    "    stacked_summary[pred_col] = get_summary_metrics(stacked_correlations[pred_col], stacked_cumsum_corrs[pred_col])\n",
    "\n",
    "stacked_summary_df = pd.DataFrame(stacked_summary).T\n",
    "display(stacked_summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "era_invariant_md"
   },
   "source": [
    "## 6. Era-Invariant Training (PyTorch MLP Option)\n",
    "\n",
    "Define and train a PyTorch MLP with a custom loss function incorporating negative Pearson correlation, era correlation variance penalty, and feature exposure penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "era_invariant_code"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "# --- Define MLP Architecture ---\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid() # Output between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# --- Define Custom Loss Functions ---\n",
    "def pearson_corr(preds, target):\n",
    "    \"\"\"Calculate Pearson correlation coefficient.\"\"\"\n",
    "    preds_mean = torch.mean(preds)\n",
    "    target_mean = torch.mean(target)\n",
    "    cov = torch.mean((preds - preds_mean) * (target - target_mean))\n",
    "    preds_std = torch.std(preds)\n",
    "    target_std = torch.std(target)\n",
    "    # Add epsilon to prevent division by zero\n",
    "    epsilon = 1e-6\n",
    "    corr = cov / (preds_std * target_std + epsilon)\n",
    "    return corr\n",
    "\n",
    "def era_correlation_variance_penalty(preds, target, eras):\n",
    "    \"\"\"Calculate variance of per-era correlations.\"\"\"\n",
    "    unique_eras = torch.unique(eras)\n",
    "    era_corrs = []\n",
    "    for era in unique_eras:\n",
    "        era_mask = (eras == era)\n",
    "        era_preds = preds[era_mask]\n",
    "        era_target = target[era_mask]\n",
    "        if len(era_preds) > 1: # Need at least 2 points for correlation\n",
    "            era_corrs.append(pearson_corr(era_preds, era_target))\n",
    "    \n",
    "    if len(era_corrs) > 1:\n",
    "        era_corrs_tensor = torch.stack(era_corrs)\n",
    "        # Handle potential NaNs from zero std dev within an era\n",
    "        valid_corrs = era_corrs_tensor[~torch.isnan(era_corrs_tensor)]\n",
    "        if len(valid_corrs) > 1:\n",
    "             return torch.var(valid_corrs)\n",
    "    return torch.tensor(0.0, device=preds.device) # Return 0 if variance can't be calculated\n",
    "\n",
    "def feature_exposure_penalty(preds, features):\n",
    "    \"\"\"Calculate sum of squared correlations between predictions and features.\"\"\"\n",
    "    num_features = features.shape[1]\n",
    "    feature_corrs_sq = []\n",
    "    for i in range(num_features):\n",
    "        feature_col = features[:, i]\n",
    "        if torch.std(feature_col) > 1e-6: # Check for constant features\n",
    "             corr = pearson_corr(preds.squeeze(), feature_col)\n",
    "             if not torch.isnan(corr):\n",
    "                 feature_corrs_sq.append(corr**2)\n",
    "        \n",
    "    if len(feature_corrs_sq) > 0:\n",
    "        return torch.mean(torch.stack(feature_corrs_sq))\n",
    "    return torch.tensor(0.0, device=preds.device)\n",
    "\n",
    "# --- Training Loop ---\n",
    "def train_mlp(train_df, feature_cols, target_col, era_col, top_n_features=TOP_N_FEATURES_FOR_EXPOSURE):\n",
    "    print(\"\\nTraining PyTorch MLP with custom loss...\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Prepare data\n",
    "    train_target_filtered = train_df.dropna(subset=[target_col])\n",
    "    features = torch.tensor(train_target_filtered[feature_cols].fillna(0.5).values, dtype=torch.float32).to(device)\n",
    "    target = torch.tensor(train_target_filtered[target_col].values, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    eras = torch.tensor(train_target_filtered[era_col].astype(int).values, dtype=torch.long).to(device)\n",
    "    \n",
    "    # Select top N features for feature exposure penalty (based on correlation with target)\n",
    "    feature_corrs = train_target_filtered[feature_cols].corrwith(train_target_filtered[target_col])\n",
    "    top_feature_indices = feature_corrs.abs().nlargest(top_n_features).index\n",
    "    top_features_tensor = torch.tensor(train_target_filtered[top_feature_indices].fillna(0.5).values, dtype=torch.float32).to(device)\n",
    "    print(f\"Using top {len(top_feature_indices)} features for exposure penalty.\")\n",
    "\n",
    "    dataset = TensorDataset(features, target, eras, top_features_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=MLP_BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    # Initialize model, optimizer, loss weights\n",
    "    input_dim = len(feature_cols)\n",
    "    mlp_model = SimpleMLP(input_dim).to(device)\n",
    "    optimizer = optim.Adam(mlp_model.parameters(), lr=MLP_LR)\n",
    "    lambda1 = VARIANCE_PENALTY_WEIGHT\n",
    "    lambda2 = FEATURE_EXPOSURE_WEIGHT\n",
    "\n",
    "    # Training\n",
    "    mlp_model.train()\n",
    "    for epoch in range(MLP_EPOCHS):\n",
    "        epoch_loss = 0.0\n",
    "        for batch_features, batch_target, batch_eras, batch_top_features in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            preds = mlp_model(batch_features)\n",
    "\n",
    "            # Calculate loss components\n",
    "            corr_loss = -pearson_corr(preds, batch_target)\n",
    "            var_penalty = era_correlation_variance_penalty(preds, batch_target, batch_eras)\n",
    "            exposure_penalty = feature_exposure_penalty(preds, batch_top_features)\n",
    "\n",
    "            # Combine losses\n",
    "            total_loss = corr_loss + lambda1 * var_penalty + lambda2 * exposure_penalty\n",
    "            \n",
    "            # Handle potential NaN loss\n",
    "            if torch.isnan(total_loss):\n",
    "                print(f\"Warning: NaN loss encountered in epoch {epoch+1}. Skipping batch.\")\n",
    "                continue\n",
    "\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += total_loss.item()\n",
    "        \n",
    "        avg_epoch_loss = epoch_loss / len(dataloader)\n",
    "        print(f\"Epoch [{epoch+1}/{MLP_EPOCHS}], Loss: {avg_epoch_loss:.6f}\")\n",
    "        \n",
    "    print(\"MLP training finished.\")\n",
    "    return mlp_model.to('cpu') # Move model back to CPU for prediction\n",
    "\n",
    "# --- Control Flow for Model Training ---\n",
    "mlp_model = None\n",
    "if USE_MLP:\n",
    "    # Ensure PyTorch and dependencies are installed\n",
    "    try:\n",
    "        import torch\n",
    "        mlp_model = train_mlp(train, feature_cols, MAIN_TARGET, ERA_COL)\n",
    "        \n",
    "        # Generate MLP predictions on validation set\n",
    "        print(\"Generating MLP predictions on validation set...\")\n",
    "        mlp_model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_features_tensor = torch.tensor(validation[feature_cols].fillna(0.5).values, dtype=torch.float32)\n",
    "            mlp_preds = mlp_model(val_features_tensor).numpy().squeeze()\n",
    "        validation[\"prediction_mlp\"] = mlp_preds\n",
    "        print(\"MLP predictions generated.\")\n",
    "        display(validation[[\"prediction_mlp\"]].head())\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"PyTorch not found. Skipping MLP training. Set USE_MLP=False or install PyTorch.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during MLP training or prediction: {e}\")\n",
    "        mlp_model = None # Ensure mlp_model is None if training failed\n",
    "elif not USE_STACKING:\n",
    "    print(\"Skipping Stacking and MLP training based on configuration.\")\n",
    "    print(\"Using the base model for the main target as the final prediction.\")\n",
    "    validation[\"prediction_final\"] = validation[f\"prediction_{MAIN_TARGET}\"]\n",
    "else:\n",
    "    print(\"Using Stacked Ensemble based on configuration.\")\n",
    "    validation[\"prediction_final\"] = validation[\"prediction_stacked\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "final_eval_md"
   },
   "source": [
    "## 7. Final Model Evaluation\n",
    "\n",
    "Evaluate the chosen final model (either Stacked Ensemble or MLP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "final_eval_code"
   },
   "outputs": [],
   "source": [
    "print(\"Evaluating final model performance...\")\n",
    "\n",
    "final_pred_col = \"prediction_mlp\" if USE_MLP and mlp_model else (\"prediction_stacked\" if USE_STACKING else f\"prediction_{MAIN_TARGET}\")\n",
    "\n",
    "if final_pred_col not in validation.columns:\n",
    "    print(f\"Warning: Final prediction column '{final_pred_col}' not found. Defaulting to main target model.\")\n",
    "    final_pred_col = f\"prediction_{MAIN_TARGET}\"\n",
    "    if final_pred_col not in validation.columns:\n",
    "         raise ValueError(\"Could not find a valid prediction column for final evaluation.\")\n",
    "\n",
    "print(f\"Evaluating column: {final_pred_col}\")\n",
    "\n",
    "final_evaluation_cols = [final_pred_col]\n",
    "if USE_STACKING:\n",
    "    final_evaluation_cols = evaluation_cols # Show comparison if stacking was run\n",
    "elif USE_MLP and mlp_model:\n",
    "    final_evaluation_cols = [f\"prediction_{MAIN_TARGET}\"] + [final_pred_col] # Compare MLP to base\n",
    "\n",
    "\n",
    "final_correlations = validation.groupby(ERA_COL).apply(\n",
    "    lambda d: numerai_corr(d[final_evaluation_cols], d[MAIN_TARGET])\n",
    ")\n",
    "final_cumsum_corrs = final_correlations.cumsum()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "final_cumsum_corrs.plot(ax=plt.gca())\n",
    "plt.title(f\"Cumulative Correlation of Final Model ({final_pred_col}) vs Others\")\n",
    "plt.xlabel(\"Era\")\n",
    "plt.ylabel(\"Cumulative Correlation\")\n",
    "plt.xticks([])\n",
    "plt.legend(title=\"Model\")\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFinal Summary Metrics:\")\n",
    "final_summary = {}\n",
    "for pred_col in final_evaluation_cols:\n",
    "    final_summary[pred_col] = get_summary_metrics(final_correlations[pred_col], final_cumsum_corrs[pred_col])\n",
    "\n",
    "final_summary_df = pd.DataFrame(final_summary).T\n",
    "display(final_summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_upload_md"
   },
   "source": [
    "## 8. Model Upload\n",
    "\n",
    "Define the final prediction function based on the selected model (Stacking or MLP) and prepare for upload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WWdOTGy4xnNq"
   },
   "outputs": [],
   "source": [
    "# --- Define Final Prediction Function ---\n",
    "def predict_final(live_features: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Generates predictions using the chosen final model.\"\"\"\n",
    "    \n",
    "    # Apply feature engineering transformations to live features\n",
    "    # Important: Use transform methods of fitted transformers (UMAP, AE encoder, Scaler) \n",
    "    # and pre-calculated values (CTGAN synthetic mean) from training.\n",
    "    # This requires saving these objects during training.\n",
    "    # For this example, we'll re-run the generation functions, which is NOT best practice \n",
    "    # for production but simpler for this notebook structure.\n",
    "    print(\"Applying feature engineering to live data...\")\n",
    "    live_features_eng = live_features.copy()\n",
    "    live_features_eng, _ = umap_feature_creation(live_features_eng, original_feature_cols, n_components=UMAP_N_COMPONENTS)\n",
    "    \n",
    "    live_data_ae = live_features_eng[original_feature_cols].astype(np.float32).fillna(0.5).values\n",
    "    ae_features_live = encoder.predict(live_data_ae)\n",
    "    live_features_eng[ae_feats] = ae_features_live\n",
    "    \n",
    "    live_features_eng, _ = contrastive_feature_creation(live_features_eng, original_feature_cols, embedding_dim=CONTRASTIVE_EMB_DIM)\n",
    "    \n",
    "    if ctgan_feats: # Only if CTGAN succeeded on train data\n",
    "        live_features_original = live_features_eng[original_feature_cols].fillna(0.5).values\n",
    "        distances_live = np.linalg.norm(live_features_original - synthetic_mean.values.astype(np.float32), axis=1)\n",
    "        live_features_eng[ctgan_feats[0]] = distances_live\n",
    "    \n",
    "    print(\"Feature engineering applied to live data.\")\n",
    "\n",
    "    if USE_MLP and mlp_model:\n",
    "        print(\"Generating predictions using MLP model...\")\n",
    "        mlp_model.eval()\n",
    "        with torch.no_grad():\n",
    "            live_features_tensor = torch.tensor(live_features_eng[feature_cols].fillna(0.5).values, dtype=torch.float32)\n",
    "            predictions = mlp_model(live_features_tensor).numpy().squeeze()\n",
    "        submission_df = pd.DataFrame({'prediction': predictions}, index=live_features.index)\n",
    "        \n",
    "    elif USE_STACKING:\n",
    "        print(\"Generating predictions using Stacked Ensemble...\")\n",
    "        # Generate base model predictions on live data\n",
    "        base_preds_live = pd.DataFrame(index=live_features.index)\n",
    "        for target_name, model in models.items():\n",
    "            base_preds_live[f\"oof_{target_name}\"] = model.predict(live_features_eng[feature_cols])\n",
    "        \n",
    "        # Generate meta-model predictions\n",
    "        if STACKING_MODEL_TYPE == 'Linear':\n",
    "             base_preds_live_scaled = scaler.transform(base_preds_live)\n",
    "             stacked_preds_live = meta_model.predict(base_preds_live_scaled)\n",
    "        else: # LGBM\n",
    "             stacked_preds_live = meta_model.predict(base_preds_live)\n",
    "        \n",
    "        submission_df = pd.DataFrame({'prediction': stacked_preds_live}, index=live_features.index)\n",
    "\n",
    "    else: # Default to main target base model if others are disabled/failed\n",
    "        print(f\"Generating predictions using base model for {MAIN_TARGET}...\")\n",
    "        predictions = models[MAIN_TARGET].predict(live_features_eng[feature_cols])\n",
    "        submission_df = pd.DataFrame({'prediction': predictions}, index=live_features.index)\n",
    "\n",
    "    # Rank predictions for submission\n",
    "    ranked_submission = submission_df['prediction'].rank(pct=True, method=\"first\")\n",
    "    return ranked_submission.to_frame(PREDICTION_COL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 473
    },
    "id": "kPq_ATf0xnNr",
    "outputId": "dc1b5ed0-e3d6-4d19-ba95-58d3a7c5f44b"
   },
   "outputs": [],
   "source": [
    "# --- Quick Test on Live Data ---\n",
    "print(\"Downloading live features for testing...\")\n",
    "napi.download_dataset(f\"{DATA_VERSION}/live.parquet\")\n",
    "live_features = pd.read_parquet(f\"{DATA_VERSION}/live.parquet\", columns=original_feature_cols)\n",
    "\n",
    "# Generate predictions using the final function\n",
    "final_predictions = predict_final(live_features)\n",
    "\n",
    "print(\"\\nSample of final predictions:\")\n",
    "display(final_predictions.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pickle_upload_code"
   },
   "outputs": [],
   "source": [
    "# --- Pickle the Prediction Function and Dependencies ---\n",
    "# Important: Ensure all necessary objects (models, transformers, etc.) \n",
    "# are either defined within predict_final or are globally accessible \n",
    "# AND will be pickled by cloudpickle.\n",
    "\n",
    "# For this example, we rely on global models, encoder, scaler, synthetic_mean\n",
    "# A more robust approach would pass these as arguments or load them within the function.\n",
    "\n",
    "print(\"Pickling the prediction function...\")\n",
    "try:\n",
    "    # Ensure necessary global objects are available if not defined inside predict_final\n",
    "    global_dependencies = {\n",
    "        'models': models,\n",
    "        'encoder': encoder if 'encoder' in globals() else None, # Keras AE encoder\n",
    "        'scaler': scaler if STACKING_MODEL_TYPE == 'Linear' and 'scaler' in globals() else None, # Scaler for linear meta-model\n",
    "        'meta_model': meta_model if USE_STACKING and 'meta_model' in globals() else None, # Stacking meta-model\n",
    "        'mlp_model': mlp_model if USE_MLP and mlp_model else None, # PyTorch MLP model\n",
    "        'original_feature_cols': original_feature_cols,\n",
    "        'engineered_feature_cols': engineered_feature_cols,\n",
    "        'feature_cols': feature_cols,\n",
    "        'ae_feats': ae_feats if 'ae_feats' in globals() else [],\n",
    "        'contrastive_feats': contrastive_feats if 'contrastive_feats' in globals() else [],\n",
    "        'ctgan_feats': ctgan_feats if 'ctgan_feats' in globals() else [],\n",
    "        'synthetic_mean': synthetic_mean if 'synthetic_mean' in globals() and ctgan_feats else None, # Mean from CTGAN\n",
    "        'UMAP_N_COMPONENTS': UMAP_N_COMPONENTS,\n",
    "        'AE_ENCODING_DIM': AE_ENCODING_DIM,\n",
    "        'CONTRASTIVE_EMB_DIM': CONTRASTIVE_EMB_DIM,\n",
    "        'USE_MLP': USE_MLP,\n",
    "        'USE_STACKING': USE_STACKING,\n",
    "        'STACKING_MODEL_TYPE': STACKING_MODEL_TYPE,\n",
    "        'MAIN_TARGET': MAIN_TARGET,\n",
    "        'PREDICTION_COL': PREDICTION_COL\n",
    "    }\n",
    "    \n",
    "    # Include the feature engineering functions themselves if they are not standard library imports\n",
    "    global_dependencies['umap_feature_creation'] = umap_feature_creation\n",
    "    global_dependencies['denoising_autoencoder_features'] = denoising_autoencoder_features\n",
    "    global_dependencies['contrastive_feature_creation'] = contrastive_feature_creation\n",
    "    global_dependencies['synthetic_data_ctgan'] = synthetic_data_ctgan\n",
    "    \n",
    "    # Pickle the function along with its dependencies\n",
    "    cloudpickle.register_pickle_by_value(umap)\n",
    "    cloudpickle.register_pickle_by_value(tf)\n",
    "    cloudpickle.register_pickle_by_value(torch)\n",
    "    cloudpickle.register_pickle_by_value(ctgan)\n",
    "    \n",
    "    with open(\"predict_final_model.pkl\", \"wb\") as f:\n",
    "        cloudpickle.dump({'predict_fn': predict_final, 'dependencies': global_dependencies}, f)\n",
    "    print(\"Prediction function pickled successfully to predict_final_model.pkl\")\n",
    "\n",
    "except NameError as e:\n",
    "    print(f\"Pickling failed: A required object might not be defined. Error: {e}\")\n",
    "    print(\"Ensure all models and transformers used in 'predict_final' are trained and available globally.\")\n",
    "except Exception as e:\n",
    "     print(f\"An unexpected error occurred during pickling: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "download_final_pkl"
   },
   "outputs": [],
   "source": [
    "# Download file if running in Google Colab\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download('predict_final_model.pkl')\n",
    "except ImportError:\n",
    "    print(\"Skipping download (not in Colab environment).\")\n",
    "except Exception as e:\n",
    "    print(f\"File download failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jxO2YzmIxnNr"
   },
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "This notebook demonstrated adding feature engineering, stacked ensembling, and an optional era-invariant MLP training pipeline to the original target ensembling notebook.\n",
    "\n",
    "Remember to choose the model (Stacking or MLP) you want to submit by setting the `USE_STACKING` or `USE_MLP` flags before pickling and uploading `predict_final_model.pkl` to [numer.ai](https://numer.ai)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
